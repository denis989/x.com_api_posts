{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN91aLV9suhaRpgqMiJO32Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denis989/x.com_api_posts/blob/main/FIMI_post_download_fork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Universal Setup Block for FIMI Twitter Project**"
      ],
      "metadata": {
        "id": "ijrSkGPNLrsW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjajziLpLnSA",
        "outputId": "23595e3a-65fb-474a-f0fe-801e400166b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing necessary libraries...\n",
            "Libraries installed.\n",
            "Importing modules...\n",
            "Modules imported.\n",
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n",
            "Base data path: /content/drive/My Drive/ENC/Data/\n",
            "Logs and base Actors directories ensured.\n",
            "Bearer Token loaded successfully.\n",
            "Tweepy Client initialized (wait_on_rate_limit=True).\n",
            "\n",
            "=== Setup Complete ===\n",
            "✅ Client initialized and ready.\n"
          ]
        }
      ],
      "source": [
        "# === Universal Setup Block for FIMI Twitter Project ===\n",
        "\n",
        "# 1. Install Libraries\n",
        "print(\"Installing necessary libraries...\")\n",
        "!pip install tweepy pandas numpy -q # Added numpy explicitly\n",
        "print(\"Libraries installed.\")\n",
        "\n",
        "# 2. Import Modules\n",
        "print(\"Importing modules...\")\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "from google.colab import drive\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import matplotlib.pyplot as plt # Keep for potential quick plots\n",
        "import matplotlib.dates as mdates # Keep for potential quick plots\n",
        "import re # For filename sanitization\n",
        "import traceback # For detailed error logging\n",
        "print(\"Modules imported.\")\n",
        "\n",
        "# 3. Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "    drive_mounted = True\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "    drive_mounted = False\n",
        "\n",
        "# 4. Define Core Paths\n",
        "BASE_DRIVE_PATH = None\n",
        "KEYS_PATH = None\n",
        "LOGS_PATH = None\n",
        "ACTORS_BASE_DIR = None # Base for /actors/ structure\n",
        "\n",
        "if drive_mounted:\n",
        "    BASE_DRIVE_PATH = \"/content/drive/My Drive/ENC/Data/\" # Standardized\n",
        "    KEYS_PATH = os.path.join(BASE_DRIVE_PATH, \"keys\")\n",
        "    LOGS_PATH = os.path.join(BASE_DRIVE_PATH, \"logs\")\n",
        "    ACTORS_BASE_DIR = os.path.join(BASE_DRIVE_PATH, \"actors\") # For structured output\n",
        "    TOKEN_FILE_PATH = os.path.join(KEYS_PATH, \"bearer_token.txt\")\n",
        "    print(f\"Base data path: {BASE_DRIVE_PATH}\")\n",
        "    try:\n",
        "        os.makedirs(LOGS_PATH, exist_ok=True)\n",
        "        os.makedirs(ACTORS_BASE_DIR, exist_ok=True) # Ensure base actors dir exists\n",
        "        print(\"Logs and base Actors directories ensured.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not create core directories: {e}\")\n",
        "else:\n",
        "    print(\"Skipping path definitions as Drive is not mounted.\")\n",
        "\n",
        "# 5. Function to Load Bearer Token\n",
        "def load_bearer_token(file_path):\n",
        "    if not file_path or not os.path.exists(file_path):\n",
        "        print(f\"Error: Token file not found at '{file_path}'.\"); return None\n",
        "    try:\n",
        "        with open(file_path, 'r') as f: bearer_token = f.read().strip()\n",
        "        if not bearer_token: print(f\"Error: Token file '{file_path}' is empty.\"); return None\n",
        "        print(\"Bearer Token loaded successfully.\")\n",
        "        return bearer_token\n",
        "    except Exception as e: print(f\"An error occurred reading token: {e}\"); return None\n",
        "\n",
        "# 6. Load Token and Initialize Tweepy Client\n",
        "bearer_token = None\n",
        "client = None\n",
        "if drive_mounted and TOKEN_FILE_PATH:\n",
        "    bearer_token = load_bearer_token(TOKEN_FILE_PATH)\n",
        "    if bearer_token:\n",
        "        try:\n",
        "            client = tweepy.Client(bearer_token=bearer_token, wait_on_rate_limit=True)\n",
        "            print(\"Tweepy Client initialized (wait_on_rate_limit=True).\")\n",
        "        except Exception as e: print(f\"Error initializing Tweepy Client: {e}\"); client = None\n",
        "    else: print(\"Bearer token not loaded. Client not initialized.\")\n",
        "else: print(\"Drive not mounted or token path not set. Client not initialized.\")\n",
        "\n",
        "# 7. Function to Write Logs\n",
        "def write_log_json(log_data, log_directory):\n",
        "    if not log_directory or not os.path.exists(log_directory):\n",
        "        print(f\"Error: Log directory '{log_directory}' not valid. Cannot write log.\"); return None\n",
        "    try:\n",
        "        ts_iso = log_data.get(\"run_start_timestamp_utc\", datetime.now(timezone.utc).isoformat(timespec='seconds'))\n",
        "        # Ensure Z is appended if not present from isoformat()\n",
        "        if not ts_iso.endswith(\"Z\"): ts_iso = ts_iso.split('+')[0] + \"Z\"\n",
        "\n",
        "        ts_filename = ts_iso.replace(':','').replace('-','').replace('T','_').replace('Z','')\n",
        "        script_name_safe = log_data.get(\"script_name\", \"unknown_script\").replace('.ipynb','').replace(':','-')[:50] # Made safer\n",
        "        log_filename = f\"{ts_filename}_{script_name_safe}_run_log.json\"\n",
        "        log_filepath = os.path.join(log_directory, log_filename)\n",
        "        with open(log_filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(log_data, f, ensure_ascii=False, indent=2) # Changed to indent=2\n",
        "        print(f\"Log successfully saved to: {log_filepath}\")\n",
        "        return log_filepath\n",
        "    except Exception as e: print(f\"Error writing log file: {e}\"); return None\n",
        "\n",
        "print(\"\\n=== Setup Complete ===\")\n",
        "if client: print(\"✅ Client initialized and ready.\")\n",
        "else: print(\"❌ Client FAILED to initialize. Check Drive mount and token file.\")\n",
        "# === END OF SETUP BLOCK ==="
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advanced Tweet Collector with Estimation ver. 2**"
      ],
      "metadata": {
        "id": "pSMHNKdl1imm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "from google.colab import drive\n",
        "from datetime import datetime, timezone, timedelta, date as py_date\n",
        "import re\n",
        "import traceback\n",
        "import math\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import VBox, HBox, Layout\n",
        "from IPython.display import display, clear_output, HTML as IPHTML\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- UI Elements (Define status_html early) ---\n",
        "status_html = widgets.HTML(value=\"<i>Status updates...</i>\", layout=Layout(height='100px', overflow_y='auto', border='1px solid lightgrey', padding='5px', width='95%'))\n",
        "_status_lines = [] # Buffer for general status messages\n",
        "_token_status_line = \"\" # Dedicated line for token status\n",
        "_current_active_token = None # Track which token is currently active\n",
        "\n",
        "def update_status_display_fn(new_messages_list, is_token_status_update=False):\n",
        "    global _status_lines, _token_status_line, status_html\n",
        "\n",
        "    if is_token_status_update:\n",
        "        if isinstance(new_messages_list, list): _token_status_line = new_messages_list[0]\n",
        "        else: _token_status_line = new_messages_list\n",
        "    else:\n",
        "        if isinstance(new_messages_list, str): new_messages_list = [new_messages_list]\n",
        "        _status_lines.extend(new_messages_list)\n",
        "        _status_lines = _status_lines[-3:] # Keep last 3 general status lines\n",
        "\n",
        "    combined_status_display = f\"<b>Token Status:</b> {_token_status_line if _token_status_line else 'Initializing...'}\\n<hr>\" + \"\\n\".join(_status_lines)\n",
        "    status_html.value = \"<pre style='white-space: pre-wrap; word-wrap: break-word;'>\" + combined_status_display + \"</pre>\"\n",
        "\n",
        "# --- Global Client and Path Definitions ---\n",
        "if 'BASE_DRIVE_PATH' not in globals() or BASE_DRIVE_PATH is None:\n",
        "    update_status_display_fn([\"Warning: BASE_DRIVE_PATH not found. Fallback for paths.\"])\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        BASE_DRIVE_PATH = \"/content/drive/My Drive/ENC/Data/\"\n",
        "        KEYS_PATH = os.path.join(BASE_DRIVE_PATH, \"keys\")\n",
        "        LOGS_PATH = os.path.join(BASE_DRIVE_PATH, \"logs\")\n",
        "        ACTORS_BASE_DIR = os.path.join(BASE_DRIVE_PATH, \"actors\")\n",
        "        os.makedirs(LOGS_PATH, exist_ok=True); os.makedirs(ACTORS_BASE_DIR, exist_ok=True)\n",
        "    except Exception as e:\n",
        "        update_status_display_fn([f\"Critical Error: Fallback path setup failed: {e}\"])\n",
        "        BASE_DRIVE_PATH = None; KEYS_PATH = None\n",
        "        LOGS_PATH = \"./local_logs\"; ACTORS_BASE_DIR = \"./local_actors_data\"\n",
        "        os.makedirs(LOGS_PATH, exist_ok=True); os.makedirs(ACTORS_BASE_DIR, exist_ok=True)\n",
        "\n",
        "# --- Multi-Client Management ---\n",
        "clients_manager = {}\n",
        "current_client_index_global = -1\n",
        "\n",
        "# Phase 4: Rate limit optimization variables\n",
        "rate_limit_optimization_enabled = True\n",
        "estimated_requests_per_task = {}\n",
        "optimal_pause_duration = {}\n",
        "\n",
        "def _get_token_status_string_fn():\n",
        "    \"\"\"Helper to generate the token status summary string with color coding.\"\"\"\n",
        "    global _current_active_token\n",
        "    if not clients_manager: return \"No tokens initialized.\"\n",
        "    status_parts = []\n",
        "    now_utc = datetime.now(timezone.utc)\n",
        "\n",
        "    for token_id_str_val in sorted(clients_manager.keys()):\n",
        "        info = clients_manager[token_id_str_val]\n",
        "\n",
        "        # Determine color and status\n",
        "        if token_id_str_val == _current_active_token:\n",
        "            # Green for active token\n",
        "            color = \"#00AA00\"\n",
        "            # Show remaining requests for active token\n",
        "            if 'limits' in info:\n",
        "                # Check both types of limits\n",
        "                search_limits = info['limits'].get('search_tweets', {})\n",
        "                count_limits = info['limits'].get('counts', {})\n",
        "\n",
        "                # Показываем оба лимита если доступны\n",
        "                limits_str = []\n",
        "                if search_limits.get('remaining') is not None:\n",
        "                    limits_str.append(f\"S:{search_limits.get('remaining', '?')}/{search_limits.get('limit', '?')}\")\n",
        "                if count_limits.get('remaining') is not None:\n",
        "                    limits_str.append(f\"C:{count_limits.get('remaining', '?')}/{count_limits.get('limit', '?')}\")\n",
        "\n",
        "                if limits_str:\n",
        "                    status_text = f\"Active ({', '.join(limits_str)})\"\n",
        "                else:\n",
        "                    status_text = \"Active\"\n",
        "            else:\n",
        "                status_text = \"Active\"\n",
        "        elif now_utc < info['rate_limited_until']:\n",
        "            # Red for rate-limited token with countdown\n",
        "            color = \"#FF0000\"\n",
        "            wait_secs = (info['rate_limited_until'] - now_utc).total_seconds()\n",
        "            status_text = f\"Wait {int(wait_secs)}s\"\n",
        "        else:\n",
        "            # Default color for available tokens\n",
        "            color = \"inherit\"\n",
        "            # Show limits for available tokens\n",
        "            if 'limits' in info:\n",
        "                search_limits = info['limits'].get('search_tweets', {})\n",
        "                count_limits = info['limits'].get('counts', {})\n",
        "\n",
        "                limits_str = []\n",
        "                if search_limits.get('remaining') is not None:\n",
        "                    limits_str.append(f\"S:{search_limits.get('remaining', '?')}/{search_limits.get('limit', '?')}\")\n",
        "                if count_limits.get('remaining') is not None:\n",
        "                    limits_str.append(f\"C:{count_limits.get('remaining', '?')}/{count_limits.get('limit', '?')}\")\n",
        "\n",
        "                if limits_str:\n",
        "                    status_text = f\"Ready ({', '.join(limits_str)})\"\n",
        "                else:\n",
        "                    status_text = \"Ready\"\n",
        "            else:\n",
        "                status_text = \"Ready\"\n",
        "\n",
        "        # Make text bold for active token\n",
        "        font_weight = \"bold\" if token_id_str_val == _current_active_token else \"normal\"\n",
        "        status_parts.append(f'<span style=\"color: {color}; font-weight: {font_weight};\">{token_id_str_val}: {status_text}</span>')\n",
        "\n",
        "    return \" | \".join(status_parts)\n",
        "\n",
        "def calculate_optimal_pause_fn(endpoint_type, estimated_requests, tokens_info):\n",
        "    \"\"\"\n",
        "    Phase 4.1: Calculate optimal pause duration based on rate limits.\n",
        "\n",
        "    Args:\n",
        "        endpoint_type: 'counts' or 'search'\n",
        "        estimated_requests: Number of requests expected\n",
        "        tokens_info: Dictionary with token rate limit information\n",
        "\n",
        "    Returns:\n",
        "        Optimal pause duration in seconds\n",
        "    \"\"\"\n",
        "    if not rate_limit_optimization_enabled:\n",
        "        return 0.5  # Reduced default pause\n",
        "\n",
        "    # Aggregate available rate limits across all tokens\n",
        "    total_remaining = 0\n",
        "    earliest_reset = None\n",
        "    available_tokens = 0\n",
        "\n",
        "    now_utc = datetime.now(timezone.utc)\n",
        "\n",
        "    for token_id, info in tokens_info.items():\n",
        "        # Skip tokens that are rate-limited\n",
        "        if now_utc < info['rate_limited_until']:\n",
        "            continue\n",
        "\n",
        "        available_tokens += 1\n",
        "\n",
        "        if 'limits' in info and endpoint_type in info['limits']:\n",
        "            limits = info['limits'][endpoint_type]\n",
        "            remaining = limits.get('remaining', 0)\n",
        "            reset_ts = limits.get('reset_ts', 0)\n",
        "\n",
        "            if remaining > 0:\n",
        "                total_remaining += remaining\n",
        "                if reset_ts > 0:\n",
        "                    reset_dt = datetime.fromtimestamp(reset_ts, timezone.utc)\n",
        "                    if earliest_reset is None or reset_dt < earliest_reset:\n",
        "                        earliest_reset = reset_dt\n",
        "\n",
        "    # If no limit info available, use minimal pause\n",
        "    if available_tokens == 0 or total_remaining == 0 or earliest_reset is None:\n",
        "        return 0.2  # Minimal pause when no rate limit info\n",
        "\n",
        "    # Calculate time until reset\n",
        "    time_until_reset = (earliest_reset - now_utc).total_seconds()\n",
        "    if time_until_reset <= 0:\n",
        "        return 0.1  # Very minimal pause if reset passed\n",
        "\n",
        "    # Calculate optimal pause\n",
        "    if estimated_requests > 0:\n",
        "        # If we have plenty of remaining requests, use minimal pause\n",
        "        if total_remaining > estimated_requests * 2:\n",
        "            return 0.1  # We have plenty of capacity\n",
        "\n",
        "        # Otherwise distribute requests over time\n",
        "        safe_remaining = total_remaining * 0.9  # 10% buffer\n",
        "\n",
        "        if estimated_requests >= safe_remaining:\n",
        "            # More requests than capacity - need to be careful\n",
        "            optimal_pause = time_until_reset / safe_remaining\n",
        "        else:\n",
        "            # Fewer requests than capacity - can go faster\n",
        "            optimal_pause = time_until_reset / (total_remaining * 2)\n",
        "    else:\n",
        "        # No estimate - use very small pause\n",
        "        optimal_pause = 0.2\n",
        "\n",
        "    # Apply bounds: minimum 0.1s, maximum 5s\n",
        "    optimal_pause = max(0.1, min(5.0, optimal_pause))\n",
        "\n",
        "    # Add tiny random jitter\n",
        "    jitter = np.random.uniform(-0.02, 0.02)\n",
        "    optimal_pause += max(0, jitter)\n",
        "\n",
        "    return round(optimal_pause, 2)\n",
        "\n",
        "def estimate_request_count_fn(start_date, end_date, tasks_count):\n",
        "    \"\"\"\n",
        "    Phase 4: Estimate number of API requests needed.\n",
        "\n",
        "    For counts API: ~1 request per 30-31 days\n",
        "    For search API: varies based on tweet volume\n",
        "    \"\"\"\n",
        "    days_diff = (end_date - start_date).days + 1\n",
        "\n",
        "    # Counts API: approximately 1 request per 30-31 days\n",
        "    count_requests_per_task = math.ceil(days_diff / 30.5)\n",
        "    total_count_requests = count_requests_per_task * tasks_count\n",
        "\n",
        "    # Search API: учитываем актуальный max_results\n",
        "    actual_max_results = 100 if 'context_annotations' in TWEET_FIELDS_COMPREHENSIVE else 500\n",
        "    avg_search_requests_per_task = 1  # Will be updated based on actual counts\n",
        "\n",
        "    return {\n",
        "        'count_requests_per_task': count_requests_per_task,\n",
        "        'total_count_requests': total_count_requests,\n",
        "        'search_requests_estimate': avg_search_requests_per_task,\n",
        "        'actual_max_results': actual_max_results\n",
        "    }\n",
        "\n",
        "def initialize_clients_fn():\n",
        "    global clients_manager, KEYS_PATH, current_client_index_global, _current_active_token\n",
        "    clients_manager.clear(); current_client_index_global = -1; _current_active_token = None\n",
        "    token_files = [\"bearer_token.txt\", \"bearer_token2.txt\", \"bearer_token3.txt\"]\n",
        "    loaded_tokens_count = 0\n",
        "    if KEYS_PATH is None: update_status_display_fn([\"Error: KEYS_PATH undefined.\"]); return False\n",
        "    for i, token_file in enumerate(token_files):\n",
        "        token_id = f\"Token{i+1}\"; full_token_path = os.path.join(KEYS_PATH, token_file); bearer_token_val = None\n",
        "        try:\n",
        "            if os.path.exists(full_token_path):\n",
        "                with open(full_token_path, 'r') as f_token: bearer_token_val = f_token.read().strip()\n",
        "                if bearer_token_val:\n",
        "                    client_instance = tweepy.Client(bearer_token=bearer_token_val, wait_on_rate_limit=False)\n",
        "                    clients_manager[token_id] = {\n",
        "                        'client': client_instance,\n",
        "                        'rate_limited_until': datetime.now(timezone.utc) - timedelta(seconds=1),\n",
        "                        'bearer_token_value': bearer_token_val,\n",
        "                        'last_used_timestamp': datetime.min.replace(tzinfo=timezone.utc),\n",
        "                        'limits': {},\n",
        "                        'request_count': 0  # Phase 4: Track requests per token\n",
        "                    }\n",
        "                    loaded_tokens_count +=1; update_status_display_fn([f\"Initialized {token_id}.\"])\n",
        "                else: update_status_display_fn([f\"Warn: {token_file} empty.\"])\n",
        "            else: update_status_display_fn([f\"Warn: {token_file} not found at {full_token_path}.\"])\n",
        "        except Exception as e_client_init: update_status_display_fn([f\"Err init {token_id}: {str(e_client_init)[:50]}\"])\n",
        "    update_status_display_fn(_get_token_status_string_fn(), is_token_status_update=True)\n",
        "    if loaded_tokens_count == 0: update_status_display_fn([\"Critical: No Tokens loaded.\"]); return False\n",
        "    update_status_display_fn([f\"Initialized {loaded_tokens_count} client(s).\"]); return True\n",
        "\n",
        "def get_next_available_client_fn():\n",
        "    \"\"\"\n",
        "    Возвращает:\n",
        "        active_client – объект tweepy.Client, готовый к использованию;\n",
        "        token_id_used – строковый идентификатор выбранного токена (например 'Token1').\n",
        "\n",
        "    Логика (без рекурсии):\n",
        "        1. Берём только токены, отмеченные галочками пользователем.\n",
        "        2. Отбрасываем токены, находящиеся в состоянии rate-limitʼа.\n",
        "        3. Считаем «score» (свободные лимиты + время простоя) и берём токен с\n",
        "           максимальным score.\n",
        "        4. Если все выбранные токены в rate-limitʼе – вычисляем ближайший момент\n",
        "           сброса и ОДИН раз ждём нужное время, обновляя счётчик каждую секунду.\n",
        "           После ожидания переходим к шагу 3 (всё внутри одного while-цикла).\n",
        "    \"\"\"\n",
        "    global clients_manager, current_client_index_global, _current_active_token, token_selection_checkboxes\n",
        "\n",
        "    # 1. Проверка, что клиенты вообще есть\n",
        "    if not clients_manager:\n",
        "        update_status_display_fn([\"Fatal: No clients.\"])\n",
        "        return None, None\n",
        "\n",
        "    # 2. Определяем токены, отмеченные пользователем\n",
        "    selected_tokens = [\n",
        "        tid for tid in sorted(clients_manager.keys())\n",
        "        if tid in token_selection_checkboxes and token_selection_checkboxes[tid].value\n",
        "    ]\n",
        "    if not selected_tokens:\n",
        "        update_status_display_fn([\"Error: No tokens selected for use!\"])\n",
        "        return None, None\n",
        "\n",
        "    # 3. Основной бесконечный цикл (выход происходит при успешном выборе токена\n",
        "    #    ИЛИ при непредвиденной ошибке/отсутствии данных)\n",
        "    max_wait_cycles = 0  # Защита от бесконечного ожидания\n",
        "    while max_wait_cycles < 10:  # Максимум 10 циклов ожидания\n",
        "        best_token_id = None\n",
        "        best_score    = -1\n",
        "        now_utc       = datetime.now(timezone.utc)\n",
        "\n",
        "        # 3а. Выбираем лучший доступный токен\n",
        "        for token_id in selected_tokens:\n",
        "            info = clients_manager[token_id]\n",
        "\n",
        "            # Пропускаем токены, ещё находящиеся в блокировке\n",
        "            if now_utc < info['rate_limited_until']:\n",
        "                continue\n",
        "\n",
        "            # Считаем «score»\n",
        "            score = 0\n",
        "            if 'limits' in info:\n",
        "                limits = info['limits']\n",
        "                score += limits.get('search_tweets', {}).get('remaining', 100) * 2\n",
        "                score += limits.get('counts', {}).get('remaining', 100)\n",
        "                score += limits.get('users', {}).get('remaining', 100) * 0.5\n",
        "            else:\n",
        "                score = 100  # нет данных о лимитах – даём базовый балл\n",
        "\n",
        "            # Бонус за «отдых»\n",
        "            idle_secs = (now_utc - info['last_used_timestamp']).total_seconds()\n",
        "            score += min(50, idle_secs / 60)   # до +50 баллов\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score   = score\n",
        "                best_token_id = token_id\n",
        "\n",
        "        # 3б. Если нашли подходящий токен – возвращаем его\n",
        "        if best_token_id:\n",
        "            _current_active_token = best_token_id\n",
        "            chosen_info = clients_manager[best_token_id]\n",
        "            chosen_info['last_used_timestamp'] = now_utc\n",
        "            chosen_info['request_count'] += 1\n",
        "            update_status_display_fn(_get_token_status_string_fn(), is_token_status_update=True)\n",
        "            return chosen_info['client'], best_token_id\n",
        "\n",
        "        # 3в. Если сюда дошли – ВСЕ выбранные токены всё ещё «красные».\n",
        "        #     Считаем, сколько ждать до ближайшего сброса.\n",
        "        future_resets = [\n",
        "            clients_manager[tid]['rate_limited_until']\n",
        "            for tid in selected_tokens\n",
        "            if clients_manager[tid]['rate_limited_until'] > now_utc\n",
        "        ]\n",
        "        if not future_resets:\n",
        "            # Не смогли определить время сброса (маловероятно) – выходим\n",
        "            update_status_display_fn([\"Emergency: Could not find available client.\"])\n",
        "            return None, None\n",
        "\n",
        "        earliest_reset = min(future_resets)\n",
        "        wait_seconds   = max(1, int((earliest_reset - now_utc).total_seconds()))\n",
        "\n",
        "        # Ограничиваем максимальное время ожидания\n",
        "        if wait_seconds > 900:  # 15 минут максимум\n",
        "            update_status_display_fn([\"Error: All tokens rate-limited for >15 minutes. Check API status.\"])\n",
        "            return None, None\n",
        "\n",
        "        update_status_display_fn(\n",
        "            [f\"All tokens rate-limited. Waiting ~{wait_seconds} s until reset…\"]\n",
        "        )\n",
        "\n",
        "        # 3г. «Грамотное ожидание» – показываем обратный отсчёт\n",
        "        start = time.monotonic()\n",
        "        while True:\n",
        "            elapsed   = time.monotonic() - start\n",
        "            remaining = wait_seconds - int(elapsed)\n",
        "            if remaining <= 0:\n",
        "                break\n",
        "            # Обновляем каждые 5 секунд (или чаще, если осталось мало времени)\n",
        "            update_status_display_fn([f\"Waiting: {remaining} s…\"], is_token_status_update=False)\n",
        "            time.sleep(min(5, remaining))\n",
        "\n",
        "        max_wait_cycles += 1\n",
        "        # Цикл while True продолжится и попробует выбрать токен снова\n",
        "\n",
        "    # Если достигли максимума циклов ожидания\n",
        "    update_status_display_fn([\"Error: Maximum wait cycles reached. Something is wrong with rate limits.\"])\n",
        "    return None, None\n",
        "\n",
        "def update_client_rate_limit_info_fn(token_id_val, headers_dict_val, endpoint_name_val=\"unknown\"):\n",
        "    \"\"\"Update rate limit info from response headers.\"\"\"\n",
        "    global clients_manager\n",
        "    if token_id_val not in clients_manager or not headers_dict_val:\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Extract rate limit headers\n",
        "        limit = int(headers_dict_val.get('x-rate-limit-limit', 0))\n",
        "        remaining = int(headers_dict_val.get('x-rate-limit-remaining', 0))\n",
        "        reset_ts = int(headers_dict_val.get('x-rate-limit-reset', 0))\n",
        "\n",
        "        # Determine endpoint type more accurately\n",
        "        if 'count' in endpoint_name_val.lower() or 'get_all_tweets_count' in endpoint_name_val:\n",
        "            endpoint_type = 'counts'\n",
        "        elif 'search' in endpoint_name_val.lower() or 'search_all_tweets' in endpoint_name_val:\n",
        "            endpoint_type = 'search_tweets'\n",
        "        elif 'user' in endpoint_name_val.lower():\n",
        "            endpoint_type = 'users'\n",
        "        else:\n",
        "            endpoint_type = 'unknown'\n",
        "\n",
        "        # Store limit info\n",
        "        if limit > 0:  # Only update if we got valid data\n",
        "            clients_manager[token_id_val]['limits'][endpoint_type] = {\n",
        "                'limit': limit,\n",
        "                'remaining': remaining,\n",
        "                'reset_ts': reset_ts,\n",
        "                'checked_at_utc': datetime.now(timezone.utc).isoformat()\n",
        "            }\n",
        "\n",
        "        # Update rate_limited_until if exhausted\n",
        "        if remaining == 0 and reset_ts > time.time():\n",
        "            new_limit_until = datetime.fromtimestamp(reset_ts, timezone.utc) + timedelta(seconds=10)\n",
        "            if new_limit_until > clients_manager[token_id_val]['rate_limited_until']:\n",
        "                clients_manager[token_id_val]['rate_limited_until'] = new_limit_until\n",
        "                update_status_display_fn([f\"Rate limit hit: {token_id_val} for {endpoint_type}, reset at {new_limit_until:%H:%M:%S}.\"])\n",
        "\n",
        "        # Update token status display\n",
        "        update_status_display_fn(_get_token_status_string_fn(), is_token_status_update=True)\n",
        "\n",
        "    except Exception as e_hdr_parse:\n",
        "        update_status_display_fn([f\"Warn: Parse RL headers {token_id_val}: {e_hdr_parse}\"])\n",
        "\n",
        "# --- Field Definitions (Phase 2: Reduced Media/Poll fields) ---\n",
        "TWEET_FIELDS_COMPREHENSIVE = [\n",
        "    \"created_at\", \"author_id\", \"conversation_id\", \"entities\", \"geo\",\n",
        "    \"in_reply_to_user_id\", \"lang\", \"possibly_sensitive\", \"public_metrics\",\n",
        "    \"referenced_tweets\", \"reply_settings\", \"source\", \"withheld\",\n",
        "    \"attachments\", \"edit_history_tweet_ids\",\n",
        "    # Добавленные поля:\n",
        "    \"context_annotations\",  # Семантические аннотации (важно для анализа!)\n",
        "    \"id\",                   # ID твита (обычно по умолчанию, но лучше явно указать)\n",
        "    \"text\"                  # Текст твита (обычно по умолчанию, но лучше явно указать)\n",
        "]\n",
        "EXPANSIONS_COMPREHENSIVE = [\"author_id\", \"referenced_tweets.id\", \"in_reply_to_user_id\", \"attachments.media_keys\", \"attachments.poll_ids\", \"geo.place_id\", \"entities.mentions.username\", \"referenced_tweets.id.author_id\"]\n",
        "USER_FIELDS_COMPREHENSIVE = [\"created_at\", \"description\", \"entities\", \"id\", \"location\", \"name\", \"pinned_tweet_id\", \"profile_image_url\", \"protected\", \"public_metrics\", \"url\", \"username\", \"verified\", \"verified_type\", \"withheld\"]\n",
        "MEDIA_FIELDS_COMPREHENSIVE = [\n",
        "    \"media_key\", \"type\", \"url\", \"preview_image_url\", \"duration_ms\",\n",
        "    \"height\", \"width\", \"public_metrics\", \"alt_text\",\n",
        "    # Добавленные поля:\n",
        "    \"variants\"  # Варианты для видео (разные битрейты/форматы)\n",
        "]\n",
        "POLL_FIELDS_COMPREHENSIVE = [\n",
        "    \"id\", \"options\", \"voting_status\", \"duration_minutes\", \"end_datetime\"\n",
        "    # Все основные поля уже есть - это полный набор для polls\n",
        "]\n",
        "PLACE_FIELDS_COMPREHENSIVE = [\"contained_within\", \"country\", \"country_code\", \"full_name\", \"geo\", \"id\", \"name\", \"place_type\"]\n",
        "\n",
        "# --- Other Global Variables and Helper Functions ---\n",
        "FIMI_EVENTS = [\"1. The Black Sea Grain Initiative\", \"2. Finland and Sweden's NATO Entry\", \"3. Twin Earthquake\", \"4. Middle East\", \"5. Local and National Elections\"]\n",
        "ACTORS_POSTS_LOG_DIR = None\n",
        "if 'LOGS_PATH' in globals() and LOGS_PATH is not None: ACTORS_POSTS_LOG_DIR = os.path.join(LOGS_PATH, \"actors_posts_downloads\"); os.makedirs(ACTORS_POSTS_LOG_DIR, exist_ok=True)\n",
        "else: ACTORS_POSTS_LOG_DIR = \"./local_logs/actors_posts_downloads\"; os.makedirs(ACTORS_POSTS_LOG_DIR, exist_ok=True)\n",
        "estimation_results_df = pd.DataFrame(); user_id_cache = {}; task_selection_checkboxes_global = []; user_confirmed_download_limit_global = 0\n",
        "total_posts_found_across_all_counts = 0  # Счетчик общего количества найденных постов\n",
        "\n",
        "def sanitize_filename(text_val, max_length=60):\n",
        "    if not text_val or not text_val.strip(): return \"NO_ADDITIONAL_QUERY\"\n",
        "    text_val = str(text_val); text_val = re.sub(r'[^\\w\\s-]', '', text_val); text_val = re.sub(r'[-\\s]+', '_', text_val).strip('_'); return text_val[:max_length].rstrip('_')\n",
        "\n",
        "def get_fimi_slug(fimi_event_name_str_val):\n",
        "    parts = fimi_event_name_str_val.split('.', 1); num_part = parts[0].strip(); name_part_slug = sanitize_filename(parts[1].strip() if len(parts) > 1 else fimi_event_name_str_val.strip(), max_length=100); return f\"{num_part}_{name_part_slug}\"\n",
        "\n",
        "def log_operation_fn(log_data_dict_val, stage_str_val=\"general\"):\n",
        "    global ACTORS_POSTS_LOG_DIR\n",
        "    if not ACTORS_POSTS_LOG_DIR: update_status_display_fn([\"Error: Log dir NA\"]); return None\n",
        "    try:\n",
        "        ts_iso_utc = datetime.now(timezone.utc).isoformat(timespec='seconds').replace('+00:00', 'Z'); ts_filename_part = ts_iso_utc.replace(':','').replace('-','').replace('T','_').replace('Z','')\n",
        "        script_name_part = \"actors_posts_dl_script_ph4\"; log_filename = f\"{ts_filename_part}_{script_name_part}_{stage_str_val}_log.json\"; log_filepath = os.path.join(ACTORS_POSTS_LOG_DIR, log_filename)\n",
        "        with open(log_filepath, 'w', encoding='utf-8') as f_log: json.dump(log_data_dict_val, f_log, ensure_ascii=False, indent=2)\n",
        "        return log_filepath\n",
        "    except Exception as e_log: update_status_display_fn([f\"LogWriteErr: {str(e_log)[:50]}\"]); return None\n",
        "\n",
        "def get_user_id_from_username_cached_fn(username_str_val):\n",
        "    global user_id_cache, clients_manager\n",
        "    clean_username = username_str_val.strip().lstrip('@')\n",
        "    if not clean_username: return None\n",
        "    if clean_username in user_id_cache: return user_id_cache[clean_username]\n",
        "\n",
        "    active_client, token_id_used = get_next_available_client_fn()\n",
        "    if not active_client: update_status_display_fn([\"Err: No client for User ID lookup.\"]); return None\n",
        "\n",
        "    try:\n",
        "        # Phase 4: Dynamic pause for user lookup\n",
        "        pause_duration = calculate_optimal_pause_fn('users', 1, clients_manager)\n",
        "        time.sleep(pause_duration)\n",
        "\n",
        "        user_response = active_client.get_users(usernames=[clean_username], user_fields=[\"id\"])\n",
        "\n",
        "        # Extract headers correctly\n",
        "        resp_headers = {}\n",
        "        if hasattr(user_response, '_headers'):\n",
        "            resp_headers = dict(user_response._headers)\n",
        "        elif hasattr(user_response, 'response') and hasattr(user_response.response, 'headers'):\n",
        "            resp_headers = dict(user_response.response.headers)\n",
        "\n",
        "        update_client_rate_limit_info_fn(token_id_used, resp_headers, f\"get_users_for_{clean_username}\")\n",
        "\n",
        "        if user_response.data and len(user_response.data) > 0:\n",
        "            user_id_val = user_response.data[0].id\n",
        "            user_id_cache[clean_username] = user_id_val\n",
        "            return user_id_val\n",
        "        else:\n",
        "            update_status_display_fn([f\"Warn: User '{clean_username}' not found.\"])\n",
        "            return None\n",
        "    except tweepy.TooManyRequests as tmr_user_id:\n",
        "        update_status_display_fn([f\"RL on {token_id_used} UserID lookup for {clean_username}.\"])\n",
        "        reset_time_unix = tmr_user_id.response.headers.get('x-rate-limit-reset')\n",
        "        reset_time_dt = datetime.fromtimestamp(int(reset_time_unix), timezone.utc) if reset_time_unix else datetime.now(timezone.utc) + timedelta(minutes=16)\n",
        "        clients_manager[token_id_used]['rate_limited_until'] = reset_time_dt + timedelta(seconds=10)\n",
        "        update_client_rate_limit_info_fn(token_id_used, tmr_user_id.response.headers, f\"get_users_tmr_for_{clean_username}\")\n",
        "        update_status_display_fn(_get_token_status_string_fn(), is_token_status_update=True)\n",
        "        return get_user_id_from_username_cached_fn(username_str_val)\n",
        "    except Exception as e_user_id:\n",
        "        update_status_display_fn([f\"Err get ID {clean_username}: {str(e_user_id)[:70]}\"])\n",
        "        return None\n",
        "\n",
        "def check_user_activity_in_period(username, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Быстрая проверка, есть ли у пользователя посты в указанный период\n",
        "    Returns: (has_posts, last_post_date, status_message)\n",
        "    \"\"\"\n",
        "    active_client, token_id_used = get_next_available_client_fn()\n",
        "    if not active_client:\n",
        "        return None, None, \"No client available\"\n",
        "\n",
        "    try:\n",
        "        # Быстрый запрос - берем только 1 твит\n",
        "        pause_duration = calculate_optimal_pause_fn('search_tweets', 1, clients_manager)\n",
        "        time.sleep(pause_duration)\n",
        "\n",
        "        # Запрос для получения последнего твита пользователя\n",
        "        query = f\"from:{username.lstrip('@')}\"\n",
        "        response = active_client.search_all_tweets(\n",
        "            query=query,\n",
        "            max_results=10,  # Берем 10 для надежности\n",
        "            tweet_fields=[\"created_at\"],\n",
        "            sort_order=\"recency\"  # Сортировка по новизне\n",
        "        )\n",
        "\n",
        "        # Обновляем rate limit info\n",
        "        resp_headers = {}\n",
        "        if hasattr(response, '_headers'):\n",
        "            resp_headers = dict(response._headers)\n",
        "        elif hasattr(response, 'response') and hasattr(response.response, 'headers'):\n",
        "            resp_headers = dict(response.response.headers)\n",
        "\n",
        "        update_client_rate_limit_info_fn(token_id_used, resp_headers, f\"activity_check_{username[:10]}\")\n",
        "\n",
        "        if response.data and len(response.data) > 0:\n",
        "            # Берем самый свежий твит\n",
        "            latest_tweet = response.data[0]\n",
        "            latest_date = datetime.fromisoformat(latest_tweet.created_at.replace('Z', '+00:00'))\n",
        "\n",
        "            # Проверяем, попадает ли в период\n",
        "            start_dt = datetime(start_date.year, start_date.month, start_date.day, tzinfo=timezone.utc)\n",
        "            end_dt = datetime(end_date.year, end_date.month, end_date.day, 23, 59, 59, tzinfo=timezone.utc)\n",
        "\n",
        "            if latest_date < start_dt:\n",
        "                return False, latest_date, f\"Last post before period ({latest_date.strftime('%Y-%m-%d')})\"\n",
        "            elif latest_date > end_dt:\n",
        "                # Нужно проверить, есть ли посты В периоде\n",
        "                # Делаем запрос с ограничением по датам\n",
        "                time.sleep(pause_duration)\n",
        "                period_response = active_client.search_all_tweets(\n",
        "                    query=query,\n",
        "                    start_time=start_dt.isoformat(),\n",
        "                    end_time=end_dt.isoformat(),\n",
        "                    max_results=10\n",
        "                )\n",
        "\n",
        "                if period_response.data and len(period_response.data) > 0:\n",
        "                    return True, latest_date, f\"Active in period (latest: {latest_date.strftime('%Y-%m-%d')})\"\n",
        "                else:\n",
        "                    return False, latest_date, f\"No posts in period (latest: {latest_date.strftime('%Y-%m-%d')})\"\n",
        "            else:\n",
        "                return True, latest_date, f\"Active in period (latest: {latest_date.strftime('%Y-%m-%d')})\"\n",
        "        else:\n",
        "            return False, None, \"No posts found\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, None, f\"Error checking: {str(e)[:30]}\"\n",
        "\n",
        "def build_full_search_query_fn(username_str_val, tweet_type_api_filter_str, additional_query_text_str):\n",
        "    \"\"\"\n",
        "    ИСПРАВЛЕНО: Добавлен return в конце функции\n",
        "    Правильное построение поискового запроса для Twitter API v2\n",
        "    Порядок: from:username (additional_query) filters\n",
        "    \"\"\"\n",
        "    query_parts = []\n",
        "\n",
        "    # 1. Добавляем username если есть\n",
        "    if username_str_val and username_str_val.strip():\n",
        "        query_parts.append(f\"from:{username_str_val.lstrip('@')}\")\n",
        "\n",
        "    # 2. Добавляем дополнительный поисковый запрос\n",
        "    if additional_query_text_str and additional_query_text_str.strip():\n",
        "        clean_add_q = additional_query_text_str.strip()\n",
        "        # Проверяем, нужно ли добавить скобки для группировки\n",
        "        is_already_grouped = (clean_add_q.startswith(\"(\") and clean_add_q.endswith(\")\"))\n",
        "        if is_already_grouped:\n",
        "            query_parts.append(clean_add_q)\n",
        "        elif \" OR \" in clean_add_q.upper() or (\" \" in clean_add_q and not clean_add_q.startswith('\"')):\n",
        "            # Добавляем скобки если есть OR или пробелы (но не если это точная фраза в кавычках)\n",
        "            query_parts.append(f\"({clean_add_q})\")\n",
        "        else:\n",
        "            query_parts.append(clean_add_q)\n",
        "\n",
        "    # 3. Добавляем фильтры типа твита В КОНЕЦ\n",
        "    if tweet_type_api_filter_str and tweet_type_api_filter_str.strip():\n",
        "        # Убираем лишние скобки если они есть\n",
        "        filter_str = tweet_type_api_filter_str.strip()\n",
        "        if filter_str.startswith(\"(\") and filter_str.endswith(\")\"):\n",
        "            filter_str = filter_str[1:-1]\n",
        "        query_parts.append(filter_str)\n",
        "\n",
        "    final_query = \" \".join(query_parts)\n",
        "\n",
        "    return final_query\n",
        "\n",
        "def build_batch_query_for_accounts(usernames_list, tweet_type_filter, additional_query, max_query_length=1024):\n",
        "    \"\"\"\n",
        "    Строит запрос для нескольких аккаунтов сразу с учетом максимальной длины\n",
        "    Returns: список запросов (каждый для группы аккаунтов)\n",
        "    \"\"\"\n",
        "    if not usernames_list:\n",
        "        return []\n",
        "\n",
        "    # Базовая часть запроса\n",
        "    base_parts = []\n",
        "    if additional_query and additional_query.strip():\n",
        "        clean_add_q = additional_query.strip()\n",
        "        if \" OR \" in clean_add_q.upper() or (\" \" in clean_add_q and not clean_add_q.startswith('\"')):\n",
        "            base_parts.append(f\"({clean_add_q})\")\n",
        "        else:\n",
        "            base_parts.append(clean_add_q)\n",
        "\n",
        "    if tweet_type_filter and tweet_type_filter.strip():\n",
        "        filter_str = tweet_type_filter.strip()\n",
        "        if filter_str.startswith(\"(\") and filter_str.endswith(\")\"):\n",
        "            filter_str = filter_str[1:-1]\n",
        "        base_parts.append(filter_str)\n",
        "\n",
        "    base_query = \" \".join(base_parts) if base_parts else \"\"\n",
        "    base_length = len(base_query) + 1 if base_query else 0  # +1 for space\n",
        "\n",
        "    # Группируем аккаунты\n",
        "    batches = []\n",
        "    current_batch = []\n",
        "    current_length = base_length\n",
        "\n",
        "    for username in usernames_list:\n",
        "        from_clause = f\"from:{username.lstrip('@')}\"\n",
        "        # Длина с учетом OR между аккаунтами\n",
        "        additional_length = len(from_clause) + (4 if current_batch else 0)  # \" OR \" = 4\n",
        "\n",
        "        if current_length + additional_length > max_query_length - 10:  # -10 для безопасности\n",
        "            if current_batch:\n",
        "                batches.append(current_batch)\n",
        "                current_batch = [username]\n",
        "                current_length = base_length + len(from_clause)\n",
        "            else:\n",
        "                # Один аккаунт уже превышает лимит\n",
        "                batches.append([username])\n",
        "        else:\n",
        "            current_batch.append(username)\n",
        "            current_length += additional_length\n",
        "\n",
        "    if current_batch:\n",
        "        batches.append(current_batch)\n",
        "\n",
        "    # Строим финальные запросы\n",
        "    final_queries = []\n",
        "    for batch in batches:\n",
        "        if len(batch) == 1:\n",
        "            query = build_full_search_query_fn(batch[0], tweet_type_filter, additional_query)\n",
        "        else:\n",
        "            # Объединяем через OR\n",
        "            from_parts = [f\"from:{u.lstrip('@')}\" for u in batch]\n",
        "            from_query = \"(\" + \" OR \".join(from_parts) + \")\"\n",
        "            query_parts = [from_query]\n",
        "            if base_query:\n",
        "                query_parts.append(base_query)\n",
        "            query = \" \".join(query_parts)\n",
        "        final_queries.append((query, batch))  # Возвращаем запрос и список аккаунтов в нем\n",
        "\n",
        "    return final_queries\n",
        "\n",
        "def check_existing_data_files_fn(fimi_slug_str_val, acc_name_str_val, query_tag_str_val, type_slug_str_val):\n",
        "    global ACTORS_BASE_DIR\n",
        "    if not ACTORS_BASE_DIR: return False\n",
        "    data_dir = os.path.join(ACTORS_BASE_DIR, fimi_slug_str_val, acc_name_str_val, query_tag_str_val, type_slug_str_val)\n",
        "    return os.path.isdir(data_dir) and any(f.endswith('.json') for f in os.listdir(data_dir))\n",
        "\n",
        "def get_accurate_daily_total_count_fn(base_query_str_val, start_time_api_val_dt, end_time_api_val_dt, task_display_name_val=\"Task\", estimated_requests=None):\n",
        "    global clients_manager\n",
        "    total_tweets_for_task = 0; current_pagination_token = None; days_processed_count = 0\n",
        "    total_days_in_period = (end_time_api_val_dt.date() - start_time_api_val_dt.date()).days + 1\n",
        "    requests_made = 0\n",
        "\n",
        "    with tqdm(total=total_days_in_period, desc=f\"Counts: {task_display_name_val[:20]}\", unit=\"day\", leave=False) as pbar_daily_counts:\n",
        "        while True:\n",
        "            active_client, token_id_used = get_next_available_client_fn()\n",
        "            if not active_client:\n",
        "                err_msg = f\"FATAL: No client for daily count: {task_display_name_val}\"\n",
        "                update_status_display_fn([err_msg]); return -1, err_msg\n",
        "            try:\n",
        "                # Phase 4: Calculate optimal pause\n",
        "                remaining_requests = estimated_requests - requests_made if estimated_requests else 10\n",
        "                pause_duration = calculate_optimal_pause_fn('counts', remaining_requests, clients_manager)\n",
        "                time.sleep(pause_duration)\n",
        "\n",
        "                start_time_str = start_time_api_val_dt.isoformat(timespec='seconds').replace('+00:00', 'Z')\n",
        "                end_time_str = end_time_api_val_dt.isoformat(timespec='seconds').replace('+00:00', 'Z')\n",
        "                current_day_counts_response = active_client.get_all_tweets_count(\n",
        "                    query=base_query_str_val, start_time=start_time_str, end_time=end_time_str,\n",
        "                    granularity='day', next_token=current_pagination_token\n",
        "                )\n",
        "                requests_made += 1\n",
        "\n",
        "                # Extract headers correctly\n",
        "                resp_headers = {}\n",
        "                if hasattr(current_day_counts_response, '_headers'):\n",
        "                    resp_headers = dict(current_day_counts_response._headers)\n",
        "                elif hasattr(current_day_counts_response, 'response') and hasattr(current_day_counts_response.response, 'headers'):\n",
        "                    resp_headers = dict(current_day_counts_response.response.headers)\n",
        "\n",
        "                update_client_rate_limit_info_fn(token_id_used, resp_headers, f\"counts_{task_display_name_val[:15]}\")\n",
        "\n",
        "                days_in_this_page = 0\n",
        "                if current_day_counts_response.data:\n",
        "                    days_in_this_page = len(current_day_counts_response.data)\n",
        "                    for day_data in current_day_counts_response.data: total_tweets_for_task += day_data.get('tweet_count', 0)\n",
        "\n",
        "                pbar_daily_counts.update(days_in_this_page); days_processed_count += days_in_this_page\n",
        "                pbar_daily_counts.set_postfix_str(f\"{days_processed_count}/{total_days_in_period}d. Total Est: {total_tweets_for_task} | Pause: {pause_duration}s\")\n",
        "\n",
        "                current_pagination_token = current_day_counts_response.meta.get('next_token')\n",
        "                if not current_pagination_token: break\n",
        "            except tweepy.TooManyRequests as tmr_counts:\n",
        "                update_status_display_fn([f\"RL on {token_id_used} for daily counts: {task_display_name_val}.\"])\n",
        "                reset_unix = tmr_counts.response.headers.get('x-rate-limit-reset')\n",
        "                reset_dt = datetime.fromtimestamp(int(reset_unix), timezone.utc) if reset_unix else datetime.now(timezone.utc) + timedelta(minutes=16)\n",
        "                clients_manager[token_id_used]['rate_limited_until'] = reset_dt + timedelta(seconds=10)\n",
        "                update_client_rate_limit_info_fn(token_id_used, tmr_counts.response.headers, f\"counts_tmr_{task_display_name_val[:10]}\")\n",
        "                update_status_display_fn(_get_token_status_string_fn(), is_token_status_update=True)\n",
        "                pbar_daily_counts.set_postfix_str(f\"RL on {token_id_used}, retrying counts page...\")\n",
        "                continue\n",
        "            except Exception as e_counts:\n",
        "                err_msg = f\"Err daily cnt {task_display_name_val} w/{token_id_used}: {str(e_counts)[:30]}\"\n",
        "                update_status_display_fn([err_msg]); traceback.print_exc(); return -1, err_msg\n",
        "    return total_tweets_for_task, \"Estimated (Full Daily Pag.)\"\n",
        "\n",
        "clients_fully_initialized_flag = initialize_clients_fn()\n",
        "token_selection_checkboxes = {}\n",
        "if clients_manager:\n",
        "    for token_id in sorted(clients_manager.keys()):\n",
        "        token_selection_checkboxes[token_id] = widgets.Checkbox(\n",
        "            value=True,\n",
        "            description=f\"Use {token_id}\",\n",
        "            indent=False\n",
        "        )\n",
        "\n",
        "token_selection_box = VBox(\n",
        "    [widgets.HTML(\"<b>Select Active Tokens:</b>\")] +\n",
        "    list(token_selection_checkboxes.values()),\n",
        "    layout=Layout(border='1px solid #ddd', padding='5px', margin='5px 0')\n",
        ")\n",
        "\n",
        "# --- UI Elements ---\n",
        "style = {'description_width': 'initial'}\n",
        "account_names_input = widgets.Textarea(value='XDevelopers,elonmusk', placeholder='Comma-separated Twitter usernames. Leave blank for global search if queries are provided.', description='Target Accounts:', layout=Layout(width='95%', height='50px'), style=style)\n",
        "search_queries_input = widgets.Textarea(value='', placeholder='One query per line. Blank for all user posts if accounts provided.', description='Additional Search Queries (Optional):', layout=Layout(width='95%', height='80px'), style=style)\n",
        "fimi_dropdown = widgets.Dropdown(options=FIMI_EVENTS, value=FIMI_EVENTS[0], description='Select FIMI Event Context:', layout=Layout(width='95%'), style=style)\n",
        "default_start_date_val = py_date(2022, 1, 1); default_end_date_val = py_date(2024, 12, 31)\n",
        "start_date_picker = widgets.DatePicker(description='Start Date:', value=default_start_date_val, style=style)\n",
        "end_date_picker = widgets.DatePicker(description='End Date:', value=default_end_date_val, style=style)\n",
        "date_box = HBox([start_date_picker, end_date_picker], layout=Layout(justify_content='space-around'))\n",
        "tweet_type_options_desc_map = {\n",
        "    \"Original Posts (No RT/Reply/Quote)\": \"-is:retweet -is:reply -is:quote\",  # Убрали скобки\n",
        "    \"Retweets (Native)\": \"is:retweet\",\n",
        "    \"Quote Tweets (RT with Comment)\": \"is:quote\",\n",
        "    \"Replies\": \"is:reply\"\n",
        "}\n",
        "def create_proper_slug(desc, api_filter):\n",
        "    \"\"\"Создает правильный slug для типа твитов\"\"\"\n",
        "    if \"Original Posts\" in desc:\n",
        "        return \"original_posts\"\n",
        "    else:\n",
        "        return re.sub(r'[^\\w_]', '', api_filter.replace(\":\", \"_\").replace(\"-\", \"\").replace(\" \", \"_\").lower())\n",
        "\n",
        "tweet_type_slugs_map_dict = {\n",
        "    desc: create_proper_slug(desc, api_filter)\n",
        "    for desc, api_filter in tweet_type_options_desc_map.items()\n",
        "}\n",
        "tweet_type_checkboxes_map_ui = {desc: widgets.Checkbox(value=True, description=desc, indent=False, layout=Layout(width='auto')) for desc in tweet_type_options_desc_map.keys()}\n",
        "select_all_tweet_types_cb_ui = widgets.Checkbox(value=True, description=\"Select/Deselect All Types\", indent=False)\n",
        "def on_select_all_tweet_types_changed_ui(change):\n",
        "    for cb_item in tweet_type_checkboxes_map_ui.values(): cb_item.value = change.new\n",
        "select_all_tweet_types_cb_ui.observe(on_select_all_tweet_types_changed_ui, names='value')\n",
        "\n",
        "# Phase 4: Add optimization toggle\n",
        "rate_limit_optimization_checkbox = widgets.Checkbox(value=True, description=\"Enable Smart Rate Limit Management (Phase 4)\", indent=False)\n",
        "batch_accounts_checkbox = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description=\"Batch multiple accounts in count queries (faster estimation)\",\n",
        "    indent=False,\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "check_activity_checkbox = widgets.Checkbox(\n",
        "    value=False,  # По умолчанию выключено, т.к. требует дополнительные запросы\n",
        "    description=\"Pre-check if accounts have posts in date range (uses extra API calls)\",\n",
        "    indent=False,\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "def on_rate_limit_optimization_changed(change):\n",
        "    global rate_limit_optimization_enabled\n",
        "    rate_limit_optimization_enabled = change.new\n",
        "    update_status_display_fn([f\"Rate limit optimization {'enabled' if change.new else 'disabled'}\"])\n",
        "rate_limit_optimization_checkbox.observe(on_rate_limit_optimization_changed, names='value')\n",
        "\n",
        "tweet_types_box_layout_ui = VBox([select_all_tweet_types_cb_ui] + list(tweet_type_checkboxes_map_ui.values()) + [widgets.HTML(\"<hr>\"), rate_limit_optimization_checkbox, batch_accounts_checkbox, check_activity_checkbox], layout=Layout(border='1px solid #ccc', padding='10px', margin='5px 0'))\n",
        "estimation_output_area_ui = widgets.Output(layout=Layout(border='1px solid #eee', padding='10px', margin='10px 0', width='95%', overflow_x='auto'))\n",
        "download_confirmation_dialog_output_ui = widgets.Output(layout=Layout(margin='10px 0', width='95%'))\n",
        "estimate_button_ui = widgets.Button(description=\"1. Estimate Tweet Counts\", icon='search', button_style='primary', layout=Layout(width='auto'))\n",
        "download_button_ui = widgets.Button(description=\"2. Download Selected Tweets\", icon='download', button_style='success', disabled=True, layout=Layout(width='auto'))\n",
        "reset_button_ui = widgets.Button(description=\"Reset Form\", icon='refresh', button_style='warning', layout=Layout(width='auto'))\n",
        "action_buttons_box_ui = HBox([estimate_button_ui, download_button_ui, reset_button_ui], layout=Layout(justify_content='space-around', margin='10px 0'))\n",
        "user_limit_input_widget_ui = widgets.IntText(value=1000, description='Max Tweets to Download (Total for this Run):', style=style, layout=Layout(width='auto'))\n",
        "confirm_limit_button_widget_ui = widgets.Button(description=\"Confirm Limit & Start Download\", button_style='danger', layout=Layout(width='auto'))\n",
        "\n",
        "@estimate_button_ui.on_click\n",
        "def handle_estimate_button_click_fn(b_est):\n",
        "    \"\"\"\n",
        "    FIX 29-May-2025\n",
        "    ----------------\n",
        "    • Правильно считаем количество задач в batch-режиме: теперь одна\n",
        "      «задача» = 1 батч (query × type × batch), а не (accounts × …).\n",
        "    • Прогресс-бар, оценка числа API-запросов и логика пауз теперь\n",
        "      ориентируются на реальное число батч-запросов (в вашем примере\n",
        "      4 query × 4 type = 16, а не 64).\n",
        "    • В сам запрос к API и сохранение результатов никакие изменения\n",
        "      не внесены — функционал остался тем же.\n",
        "    \"\"\"\n",
        "\n",
        "    # ---------- базовые проверки ----------\n",
        "    global estimation_results_df, download_button_ui, task_selection_checkboxes_global\n",
        "    global clients_manager, clients_fully_initialized_flag, estimated_requests_per_task\n",
        "\n",
        "    if not clients_fully_initialized_flag or not clients_manager:\n",
        "        update_status_display_fn([\"Error: Clients not initialized.\"])\n",
        "        return\n",
        "\n",
        "    download_button_ui.disabled = True\n",
        "    estimation_output_area_ui.clear_output()\n",
        "    download_confirmation_dialog_output_ui.clear_output()\n",
        "    task_selection_checkboxes_global.clear()\n",
        "    estimated_requests_per_task.clear()\n",
        "\n",
        "    update_status_display_fn(\n",
        "        [\"Starting ACCURATE tweet count estimation (fixed batch logic)…\"]\n",
        "    )\n",
        "\n",
        "    # ---------- читаем параметры из UI ----------\n",
        "    acc_names_raw_val       = account_names_input.value\n",
        "    queries_raw_list_val    = [q.strip() for q in search_queries_input.value.split('\\n')]\n",
        "    if not any(queries_raw_list_val):\n",
        "        queries_raw_list_val = [\"\"]                       # «пустой» запрос\n",
        "\n",
        "    fimi_event_sel          = fimi_dropdown.value\n",
        "    fimi_path_slug          = get_fimi_slug(fimi_event_sel)\n",
        "\n",
        "    start_date_val_dt       = start_date_picker.value\n",
        "    end_date_val_dt         = end_date_picker.value\n",
        "    start_time_dt_api_val   = datetime(\n",
        "        start_date_val_dt.year, start_date_val_dt.month, start_date_val_dt.day,\n",
        "        0, 0, 0, tzinfo=timezone.utc\n",
        "    )\n",
        "    end_time_dt_api_val     = datetime(\n",
        "        end_date_val_dt.year,   end_date_val_dt.month,   end_date_val_dt.day,\n",
        "        23, 59, 59, tzinfo=timezone.utc\n",
        "    )\n",
        "\n",
        "    selected_types_desc_list = [\n",
        "        desc for desc, cb_val in tweet_type_checkboxes_map_ui.items() if cb_val.value\n",
        "    ]\n",
        "\n",
        "    # ---------- разбираем аккаунты / режимы ----------\n",
        "    usernames_to_proc_list = []\n",
        "    is_global_search = False\n",
        "\n",
        "    if acc_names_raw_val.strip():\n",
        "        usernames_to_proc_list = [\n",
        "            name.strip() for name in acc_names_raw_val.split(',') if name.strip()\n",
        "        ]\n",
        "    elif queries_raw_list_val != [\"\"]:\n",
        "        # глобальный поиск без конкретных аккаунтов\n",
        "        is_global_search = True\n",
        "        usernames_to_proc_list = [\"GLOBAL_SEARCH\"]\n",
        "    else:\n",
        "        update_status_display_fn(\n",
        "            [\"Error: Accounts & Queries can't both be empty.\"]\n",
        "        )\n",
        "        return\n",
        "\n",
        "    if not selected_types_desc_list and not is_global_search:\n",
        "        update_status_display_fn(\n",
        "            [\"Error: At least one tweet type required for user-specific searches.\"]\n",
        "        )\n",
        "        return\n",
        "    if is_global_search and not any(q.strip() for q in queries_raw_list_val):\n",
        "        update_status_display_fn(\n",
        "            [\"Error: Search queries cannot be empty for a global search.\"]\n",
        "        )\n",
        "        return\n",
        "\n",
        "    # ---------- определяем, будет ли batch-режим ----------\n",
        "    batch_mode = (\n",
        "        batch_accounts_checkbox.value\n",
        "        and not is_global_search\n",
        "        and len(usernames_to_proc_list) > 1\n",
        "    )\n",
        "\n",
        "    # ---------- считаем точное число «задач» ----------\n",
        "    if batch_mode:\n",
        "        total_est_combos = 0\n",
        "        for q in queries_raw_list_val:\n",
        "            for t in selected_types_desc_list:\n",
        "                # число батчей для данной пары (q, t)\n",
        "                batches_cnt = len(\n",
        "                    build_batch_query_for_accounts(\n",
        "                        usernames_to_proc_list,\n",
        "                        tweet_type_options_desc_map.get(t, \"\"),\n",
        "                        q\n",
        "                    )\n",
        "                )\n",
        "                total_est_combos += batches_cnt\n",
        "    else:\n",
        "        num_type_iterations = len(selected_types_desc_list) if not is_global_search else 1\n",
        "        total_est_combos = (\n",
        "            len(usernames_to_proc_list) * len(queries_raw_list_val) * num_type_iterations\n",
        "        )\n",
        "\n",
        "    # ---------- оценка числа API-запросов ----------\n",
        "    request_estimates = estimate_request_count_fn(\n",
        "        start_date_val_dt, end_date_val_dt, total_est_combos\n",
        "    )\n",
        "    update_status_display_fn(\n",
        "        [f\"Phase 4: Estimated ≈ {request_estimates['total_count_requests']} \"\n",
        "         f\"count-API requests (with batching = {batch_mode}).\"]\n",
        "    )\n",
        "\n",
        "    # ---------- логируем старт ----------\n",
        "    log_est_start_data = {\n",
        "        \"stage\": \"est_start_accurate_ph4\",\n",
        "        \"ts_utc\": datetime.now(timezone.utc).isoformat(timespec='seconds').replace('+00:00', 'Z'),\n",
        "        \"params\": {\n",
        "            \"acc_mode\": \"GLOBAL\" if is_global_search else usernames_to_proc_list,\n",
        "            \"queries\": queries_raw_list_val,\n",
        "            \"fimi\": fimi_event_sel,\n",
        "            \"start_dt\": start_time_dt_api_val.isoformat(),\n",
        "            \"end_dt\": end_time_dt_api_val.isoformat(),\n",
        "            \"types\": selected_types_desc_list if not is_global_search else \"N/A_Global\",\n",
        "            \"batch_mode\": batch_mode\n",
        "        },\n",
        "        \"request_estimates\": request_estimates\n",
        "    }\n",
        "    log_operation_fn(log_est_start_data, \"est_start_accurate_ph4\")\n",
        "\n",
        "    # ---------- подготовка к циклу ----------\n",
        "    estimated_tasks_list = []\n",
        "    global total_posts_found_across_all_counts\n",
        "    total_posts_found_across_all_counts = 0\n",
        "\n",
        "    # основной прогресс-бар\n",
        "    with tqdm(total=total_est_combos,\n",
        "              desc=\"Overall Accurate Estimation (Ph4)\") as pbar_overall_est_ui:\n",
        "\n",
        "        # ====== 1. BATCH-режим =================================================\n",
        "        if batch_mode:\n",
        "            update_status_display_fn(\n",
        "                [\"Batching accounts for faster count estimation (fixed)…\"]\n",
        "            )\n",
        "\n",
        "            for query_text_iter_val in queries_raw_list_val:\n",
        "                query_tag_for_file_path = sanitize_filename(query_text_iter_val)\n",
        "\n",
        "                for type_desc_iter_val in selected_types_desc_list:\n",
        "                    type_api_filter_str = tweet_type_options_desc_map.get(\n",
        "                        type_desc_iter_val, \"\"\n",
        "                    )\n",
        "                    type_slug_for_file_path = tweet_type_slugs_map_dict.get(\n",
        "                        type_desc_iter_val, \"all_types\"\n",
        "                    )\n",
        "\n",
        "                    # строим батч-запросы\n",
        "                    batched_queries = build_batch_query_for_accounts(\n",
        "                        usernames_to_proc_list,\n",
        "                        type_api_filter_str,\n",
        "                        query_text_iter_val\n",
        "                    )\n",
        "\n",
        "                    for batch_num, (batch_query, accounts_in_batch) in enumerate(batched_queries, start=1):\n",
        "                        task_display_name = (\n",
        "                            f\"Batch{batch_num}_{len(accounts_in_batch)}acc_\"\n",
        "                            f\"{query_tag_for_file_path[:10]}_\"\n",
        "                            f\"{type_desc_iter_val[:10]}\"\n",
        "                        )\n",
        "                        pbar_overall_est_ui.set_postfix_str(task_display_name)\n",
        "\n",
        "                        # === вызов counts API на батч ===\n",
        "                        task_estimated_requests = request_estimates['count_requests_per_task']\n",
        "                        batch_count_val, count_status_msg_val = (\n",
        "                            get_accurate_daily_total_count_fn(\n",
        "                                batch_query,\n",
        "                                start_time_dt_api_val,\n",
        "                                end_time_dt_api_val,\n",
        "                                task_display_name,\n",
        "                                task_estimated_requests,\n",
        "                            )\n",
        "                        )\n",
        "\n",
        "                        # суммарно для трекера\n",
        "                        if batch_count_val > 0:\n",
        "                            total_posts_found_across_all_counts += batch_count_val\n",
        "                            pbar_overall_est_ui.set_description(\n",
        "                                f\"Overall Est. (Ph4) – Found: \"\n",
        "                                f\"{total_posts_found_across_all_counts:,}\"\n",
        "                            )\n",
        "\n",
        "                        # ---------- добавляем ОДНУ строку на батч ----------\n",
        "                        estimated_tasks_list.append({\n",
        "                            'Account': \", \".join(accounts_in_batch),\n",
        "                            'Accounts': \", \".join(accounts_in_batch),\n",
        "                            'Accounts_List': accounts_in_batch,          # пригодится при скачивании\n",
        "                            'FIMI_Event': fimi_event_sel,\n",
        "                            'Search_Query_Input': query_text_iter_val if query_text_iter_val else \"N/A\",\n",
        "                            'Query_Tag_For_Path': query_tag_for_file_path,\n",
        "                            'Tweet_Type_Desc': type_desc_iter_val,\n",
        "                            'Tweet_Type_Filter_API': type_api_filter_str or \"N/A\",\n",
        "                            'Tweet_Type_Slug_For_Path': type_slug_for_file_path,\n",
        "                            'Estimated_Count': batch_count_val,\n",
        "                            'Actual_API_Query_Used': batch_query,        # именно групповой запрос\n",
        "                            'Start_Time_API': start_time_dt_api_val.isoformat(timespec='seconds').replace('+00:00', 'Z'),\n",
        "                            'End_Time_API': end_time_dt_api_val.isoformat(timespec='seconds').replace('+00:00', 'Z'),\n",
        "                            'Status': count_status_msg_val,\n",
        "                            'Data_Exists_Hint': False,                   # для группового запроса не проверяем\n",
        "                            'Task_Display_Name': task_display_name,\n",
        "                        })\n",
        "\n",
        "                        # ←----------- прогресс-бар – теперь +1 (один батч)\n",
        "                        pbar_overall_est_ui.update(1)\n",
        "\n",
        "        # ====== 2. Обычный режим (по одному аккаунту) =========================\n",
        "        else:\n",
        "            # (этот блок не менялся)\n",
        "            # ------------------------------------------------------------------\n",
        "            for username_iter_val in usernames_to_proc_list:\n",
        "                user_id_for_logging = None\n",
        "                actual_username_for_query = None\n",
        "                display_account_name = username_iter_val\n",
        "\n",
        "                if username_iter_val == \"GLOBAL_SEARCH\":\n",
        "                    actual_username_for_query = None\n",
        "                else:\n",
        "                    actual_username_for_query = username_iter_val\n",
        "                    user_id_for_logging = get_user_id_from_username_cached_fn(\n",
        "                        actual_username_for_query\n",
        "                    )\n",
        "\n",
        "                for query_text_iter_val in queries_raw_list_val:\n",
        "                    query_tag_for_file_path = sanitize_filename(query_text_iter_val)\n",
        "\n",
        "                    types_to_iterate_list = (\n",
        "                        selected_types_desc_list\n",
        "                        if not is_global_search\n",
        "                        else [\"N/A (Global Query Default)\"]\n",
        "                    )\n",
        "                    for type_desc_iter_val in types_to_iterate_list:\n",
        "                        task_display_name_short = (\n",
        "                            f\"{display_account_name[:10]}_\"\n",
        "                            f\"{query_tag_for_file_path[:10]}_\"\n",
        "                            f\"{type_desc_iter_val[:10]}\"\n",
        "                        )\n",
        "                        pbar_overall_est_ui.set_postfix_str(task_display_name_short)\n",
        "\n",
        "                        type_api_filter_str = (\n",
        "                            tweet_type_options_desc_map.get(type_desc_iter_val, \"\")\n",
        "                            if type_desc_iter_val != \"N/A (Global Query Default)\"\n",
        "                            else \"\"\n",
        "                        )\n",
        "                        type_slug_for_file_path = (\n",
        "                            tweet_type_slugs_map_dict.get(type_desc_iter_val, \"all_types\")\n",
        "                            if type_desc_iter_val != \"N/A (Global Query Default)\"\n",
        "                            else \"all_types_global\"\n",
        "                        )\n",
        "\n",
        "                        full_api_query_built_str = build_full_search_query_fn(\n",
        "                            actual_username_for_query,\n",
        "                            type_api_filter_str,\n",
        "                            query_text_iter_val,\n",
        "                        )\n",
        "                        if not full_api_query_built_str:\n",
        "                            update_status_display_fn(\n",
        "                                [f\"Skipping invalid empty query for \\\"{task_display_name_short}\\\"\"]\n",
        "                            )\n",
        "                            pbar_overall_est_ui.update(1)\n",
        "                            continue\n",
        "\n",
        "                        task_estimated_requests = request_estimates['count_requests_per_task']\n",
        "                        current_estimated_count_val, count_status_msg_val = (\n",
        "                            get_accurate_daily_total_count_fn(\n",
        "                                full_api_query_built_str,\n",
        "                                start_time_dt_api_val,\n",
        "                                end_time_dt_api_val,\n",
        "                                task_display_name_short,\n",
        "                                task_estimated_requests,\n",
        "                            )\n",
        "                        )\n",
        "\n",
        "                        if current_estimated_count_val > 0:\n",
        "                            total_posts_found_across_all_counts += current_estimated_count_val\n",
        "                            pbar_overall_est_ui.set_description(\n",
        "                                f\"Overall Est. (Ph4) – Found: \"\n",
        "                                f\"{total_posts_found_across_all_counts:,}\"\n",
        "                            )\n",
        "\n",
        "                            actual_max_results = request_estimates.get('actual_max_results', 100)\n",
        "                            estimated_search_requests = math.ceil(\n",
        "                                current_estimated_count_val / actual_max_results\n",
        "                            )\n",
        "                            estimated_requests_per_task[task_display_name_short] = (\n",
        "                                estimated_search_requests\n",
        "                            )\n",
        "\n",
        "                        data_exists_hint_flag = check_existing_data_files_fn(\n",
        "                            fimi_path_slug,\n",
        "                            display_account_name,\n",
        "                            query_tag_for_file_path,\n",
        "                            type_slug_for_file_path,\n",
        "                        )\n",
        "                        if (data_exists_hint_flag and\n",
        "                                \"Error\" not in count_status_msg_val and\n",
        "                                \"Fatal\" not in count_status_msg_val):\n",
        "                            count_status_msg_val += \" (Exists?)\"\n",
        "\n",
        "                        estimated_tasks_list.append({\n",
        "                            'Account': display_account_name,\n",
        "                            'User_ID_Ref': user_id_for_logging,\n",
        "                            'FIMI_Event': fimi_event_sel,\n",
        "                            'Search_Query_Input': (\n",
        "                                query_text_iter_val if query_text_iter_val else \"N/A\"\n",
        "                            ),\n",
        "                            'Query_Tag_For_Path': query_tag_for_file_path,\n",
        "                            'Tweet_Type_Desc': type_desc_iter_val,\n",
        "                            'Tweet_Type_Filter_API': type_api_filter_str or \"N/A\",\n",
        "                            'Tweet_Type_Slug_For_Path': type_slug_for_file_path,\n",
        "                            'Estimated_Count': current_estimated_count_val,\n",
        "                            'Actual_API_Query_Used': full_api_query_built_str,\n",
        "                            'Start_Time_API': start_time_dt_api_val.isoformat(timespec='seconds').replace('+00:00', 'Z'),\n",
        "                            'End_Time_API': end_time_dt_api_val.isoformat(timespec='seconds').replace('+00:00', 'Z'),\n",
        "                            'Status': count_status_msg_val,\n",
        "                            'Data_Exists_Hint': data_exists_hint_flag,\n",
        "                            'Task_Display_Name': task_display_name_short,\n",
        "                        })\n",
        "\n",
        "                        pbar_overall_est_ui.update(1)\n",
        "\n",
        "    # ---------- формируем DataFrame результатов ----------\n",
        "    estimation_results_df = pd.DataFrame(estimated_tasks_list)\n",
        "\n",
        "    # Phase 4: Log token usage summary\n",
        "    token_usage_summary = {token_id: info['request_count'] for token_id, info in clients_manager.items()}\n",
        "\n",
        "    log_est_results_data = {\"stage\": \"est_results_display_accurate_ph4\", \"ts_utc\": datetime.now(timezone.utc).isoformat(timespec='seconds').replace('+00:00', 'Z'), \"params_used\": log_est_start_data[\"params\"], \"results_df\": estimation_results_df.to_dict(orient='records') if not estimation_results_df.empty else \"No results\", \"token_usage\": token_usage_summary}\n",
        "    log_operation_fn(log_est_results_data, \"est_results_display_accurate_ph4\")\n",
        "\n",
        "    with estimation_output_area_ui:\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        # ⇢ ИСПРАВЛЕНИЕ: убрано дублирование преобразования Estimated_Count\n",
        "        # Теперь делаем это только один раз и правильно обрабатываем отрицательные числа\n",
        "        if not estimation_results_df.empty:\n",
        "            # Преобразуем Estimated_Count в число, учитывая возможные отрицательные значения\n",
        "            def safe_extract_number(value):\n",
        "                \"\"\"Безопасно извлекает число из значения\"\"\"\n",
        "                if isinstance(value, (int, float)):\n",
        "                    return int(value)\n",
        "                str_val = str(value)\n",
        "                # Ищем числа, включая отрицательные\n",
        "                import re\n",
        "                match = re.search(r'-?\\d+', str_val)\n",
        "                if match:\n",
        "                    return int(match.group())\n",
        "                return 0\n",
        "\n",
        "            estimation_results_df['Estimated_Count'] = estimation_results_df['Estimated_Count'].apply(safe_extract_number)\n",
        "\n",
        "            # Пересортируем по обновлённым числам\n",
        "            estimation_results_df = estimation_results_df.sort_values(\n",
        "                by='Estimated_Count', ascending=False\n",
        "            )\n",
        "\n",
        "        # ⇢ 2. Отрисовываем таблицу\n",
        "        if not estimation_results_df.empty:\n",
        "            display(IPHTML(\n",
        "                \"<h4>Accurate Estimation Results (Phase 4 – Optimized):</h4>\"\n",
        "                \"<p>Review counts and select tasks for download.</p>\"\n",
        "            ))\n",
        "\n",
        "            header_lbls = [\n",
        "                \"Select\", \"Account\", \"Add. Query\",\n",
        "                \"Tweet Type\", \"Est. Count\", \"Status\"\n",
        "            ]\n",
        "            col_widths = ['7%', '15%', '25%', '20%', '10%', '23%']\n",
        "\n",
        "            header_row = HBox(\n",
        "                [\n",
        "                    widgets.Label(\n",
        "                        header_lbls[i],\n",
        "                        layout=Layout(width=col_widths[i], font_weight='bold')\n",
        "                    )\n",
        "                    for i in range(len(header_lbls))\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            ui_rows = [header_row]\n",
        "            task_selection_checkboxes_global.clear()        # сбрасываем старые чекбоксы\n",
        "\n",
        "            for df_idx, task_row in estimation_results_df.iterrows():\n",
        "                # число твитов для логики «выбрать/отключить»\n",
        "                count_val = task_row['Estimated_Count']\n",
        "                err_flag  = (\"Error\" in str(task_row['Status'])) or (\"Fatal\" in str(task_row['Status']))\n",
        "                is_task_enabled = (count_val > 0) and (not err_flag)\n",
        "\n",
        "                cb = widgets.Checkbox(\n",
        "                    value=is_task_enabled,\n",
        "                    description=\"\",\n",
        "                    indent=False,\n",
        "                    disabled=not is_task_enabled,\n",
        "                    layout=Layout(width=col_widths[0])\n",
        "                )\n",
        "                task_selection_checkboxes_global.append(\n",
        "                    {'checkbox': cb, 'task_df_index': df_idx}\n",
        "                )\n",
        "\n",
        "                row_vals = [\n",
        "                    task_row['Account'],\n",
        "                    task_row['Search_Query_Input'],\n",
        "                    task_row['Tweet_Type_Desc'],\n",
        "                    task_row['Estimated_Count'],\n",
        "                    task_row['Status']\n",
        "                ]\n",
        "\n",
        "                row_widgets = [cb] + [\n",
        "                    widgets.Label(\n",
        "                        (str(v)[:35] + \"…\") if len(str(v)) > 35 else str(v),\n",
        "                        layout=Layout(width=col_widths[i + 1])\n",
        "                    )\n",
        "                    for i, v in enumerate(row_vals)\n",
        "                ]\n",
        "                ui_rows.append(HBox(row_widgets))\n",
        "\n",
        "            display(VBox(ui_rows))\n",
        "\n",
        "            # ⇢ 3. Общее количество возможных твитов\n",
        "            # ИСПРАВЛЕНИЕ: более надёжный подсчёт\n",
        "            total_sel_tweets = 0\n",
        "            for _, row in estimation_results_df.iterrows():\n",
        "                count = row['Estimated_Count']\n",
        "                status = str(row['Status'])\n",
        "                if count > 0 and not any(err in status.lower() for err in ['error', 'fatal']):\n",
        "                    total_sel_tweets += count\n",
        "\n",
        "            display(IPHTML(\n",
        "                f\"<p style='margin-top:10px;'>\"\n",
        "                f\"<b>Total potentially downloadable tweets (accurate): \"\n",
        "                f\"{total_sel_tweets}</b></p>\"\n",
        "            ))\n",
        "            display(IPHTML(\n",
        "                \"<p><i>Phase 4: Token usage during estimation – \"\n",
        "                f\"{', '.join([f'{k}: {v} requests' for k, v in token_usage_summary.items()])}\"\n",
        "                \"</i></p>\"\n",
        "            ))\n",
        "\n",
        "            # ИСПРАВЛЕНИЕ: добавляем отладочную информацию\n",
        "            if total_sel_tweets == 0:\n",
        "                display(IPHTML(\n",
        "                    \"<p style='color:orange;'><b>Debug info:</b> No downloadable tweets found. \"\n",
        "                    f\"DataFrame has {len(estimation_results_df)} rows. \"\n",
        "                    f\"Rows with count > 0: {len(estimation_results_df[estimation_results_df['Estimated_Count'] > 0])}. \"\n",
        "                    f\"Rows without errors: {len(estimation_results_df[~estimation_results_df['Status'].str.contains('Error|Fatal', case=False, na=False)])}.</p>\"\n",
        "                ))\n",
        "\n",
        "            # Разблокируем кнопку скачивания, если есть что скачивать\n",
        "            download_button_ui.disabled = (total_sel_tweets == 0)\n",
        "            if total_sel_tweets == 0:\n",
        "                display(IPHTML(\n",
        "                    \"<p><i>No tweets available for download based on accurate estimations.</i></p>\"\n",
        "                ))\n",
        "            else:\n",
        "                display(IPHTML(\n",
        "                    \"<p style='color:green;'><b>✓ Download button is now enabled!</b></p>\"\n",
        "                ))\n",
        "\n",
        "        else:\n",
        "            display(IPHTML(\"<p>No accurate estimation data to display.</p>\"))\n",
        "            download_button_ui.disabled = True\n",
        "\n",
        "    update_status_display_fn([\"Phase 4 estimation complete with optimized rate limiting.\"])\n",
        "\n",
        "@download_button_ui.on_click\n",
        "def handle_download_button_click_fn(b_dl_ui):\n",
        "    global estimation_results_df, task_selection_checkboxes_global, user_confirmed_download_limit_global\n",
        "    selected_df_indices_vals = [item['task_df_index'] for item in task_selection_checkboxes_global if item['checkbox'].value and not item['checkbox'].disabled]\n",
        "    with download_confirmation_dialog_output_ui:\n",
        "        clear_output(wait=True)\n",
        "        if not selected_df_indices_vals: display(IPHTML(\"<p style='color:orange;'>No tasks selected for download.</p>\")); return\n",
        "        sel_tasks_to_conf_df = estimation_results_df.loc[selected_df_indices_vals].copy()\n",
        "        dlable_tasks_df = sel_tasks_to_conf_df[(sel_tasks_to_conf_df['Estimated_Count'] > 0) & (~sel_tasks_to_conf_df['Status'].str.contains(\"Err|Fatal\", case=False, na=False))].copy()\n",
        "        if dlable_tasks_df.empty: display(IPHTML(\"<p style='color:orange;'>Selected tasks not eligible for download.</p>\")); return\n",
        "        total_est_for_sel = dlable_tasks_df['Estimated_Count'].sum()\n",
        "        display(IPHTML(f\"<h4>Confirm Download Limit</h4><p>Selected tasks total: <b>{total_est_for_sel}</b> potential tweets (Accurate Estimate).</p><p>Monthly pull limit: 1,000,000. Enter max for THIS RUN:</p>\"))\n",
        "        user_limit_input_widget_ui.value = min(total_est_for_sel, 200000); user_limit_input_widget_ui.max = total_est_for_sel if total_est_for_sel > 0 else 1\n",
        "        try: confirm_limit_button_widget_ui.on_click(on_confirm_limit_button_widget_clicked_fn, remove=True)\n",
        "        except: pass\n",
        "        confirm_limit_button_widget_ui.on_click(on_confirm_limit_button_widget_clicked_fn)\n",
        "        display(HBox([user_limit_input_widget_ui, confirm_limit_button_widget_ui]))\n",
        "\n",
        "def on_confirm_limit_button_widget_clicked_fn(b_conf_limit_ui):\n",
        "    global user_confirmed_download_limit_global\n",
        "    user_confirmed_download_limit_global = user_limit_input_widget_ui.value\n",
        "    with download_confirmation_dialog_output_ui:\n",
        "        clear_output(wait=True)\n",
        "        if user_confirmed_download_limit_global <= 0: display(IPHTML(\"<p style='color:red;'>Download cancelled or zero limit.</p>\")); update_status_display_fn([\"Download aborted.\"]); return\n",
        "        display(IPHTML(f\"<p style='color:green;'>Download limit: <b>{user_confirmed_download_limit_global}</b>. Starting Phase 4 optimized download...</p>\"))\n",
        "    trigger_actual_download_process_phase4_fn()\n",
        "\n",
        "def trigger_actual_download_process_phase4_fn():\n",
        "    global estimation_results_df, user_confirmed_download_limit_global, ACTORS_BASE_DIR, task_selection_checkboxes_global, clients_manager, clients_fully_initialized_flag, estimated_requests_per_task\n",
        "    if not clients_fully_initialized_flag or not clients_manager: update_status_display_fn([\"Error: No clients for download.\"]); return\n",
        "    if user_confirmed_download_limit_global <= 0: update_status_display_fn([\"DL limit zero/negative.\"]); return\n",
        "    if not ACTORS_BASE_DIR: update_status_display_fn([\"Error: ACTORS_BASE_DIR not set.\"]); return\n",
        "\n",
        "    update_status_display_fn([f\"Init Phase 4 DL with fixed pause. Global limit: {user_confirmed_download_limit_global}.\"])\n",
        "    selected_df_indices = [item['task_df_index'] for item in task_selection_checkboxes_global if item['checkbox'].value and not item['checkbox'].disabled]\n",
        "    if not selected_df_indices: update_status_display_fn([\"No tasks selected for DL.\"]); return\n",
        "\n",
        "    tasks_df_selected = estimation_results_df.loc[selected_df_indices].copy()\n",
        "    tasks_to_process_orig = tasks_df_selected[(tasks_df_selected['Estimated_Count'] > 0) & (~tasks_df_selected['Status'].str.contains(\"Err|Fatal\", case=False, na=False))].sort_values(by='Estimated_Count', ascending=False).copy()\n",
        "    if tasks_to_process_orig.empty: update_status_display_fn([\"No selected tasks eligible for DL.\"]); return\n",
        "\n",
        "    total_tweets_downloaded_run = 0; download_log_list = []\n",
        "\n",
        "    # Определяем current_status и update_download_status ОДИН РАЗ в начале функции\n",
        "    current_status = {\n",
        "        'accounts_processed': 0,\n",
        "        'total_accounts': 0,\n",
        "        'current_account': '',\n",
        "        'tweets_downloaded': 0,\n",
        "        'total_target': user_confirmed_download_limit_global,\n",
        "        'current_type': ''\n",
        "    }\n",
        "\n",
        "    def update_download_status():\n",
        "        status_msg = f\"[{current_status['accounts_processed']}/{current_status['total_accounts']}] {current_status['current_account']}: {current_status['tweets_downloaded']} tweets\"\n",
        "        if current_status['current_type']:\n",
        "            status_msg += f\" ({current_status['current_type']})\"\n",
        "        update_status_display_fn([status_msg])\n",
        "\n",
        "    # Подробное логирование\n",
        "    detailed_log = {\n",
        "        'session_start': datetime.now(timezone.utc).isoformat(),\n",
        "        'accounts_processing_log': [],\n",
        "        'api_calls_log': [],\n",
        "        'rate_limit_events': [],\n",
        "        'errors_log': []\n",
        "    }\n",
        "\n",
        "    def log_detailed_event(event_type, details):\n",
        "        detailed_log[f'{event_type}_log'].append({\n",
        "            'timestamp': datetime.now(timezone.utc).isoformat(),\n",
        "            'details': details\n",
        "        })\n",
        "\n",
        "        # Сохраняем лог каждые 10 событий\n",
        "        if len(detailed_log.get(f'{event_type}_log', [])) % 10 == 0:\n",
        "            log_file_path = log_operation_fn(detailed_log, f\"detailed_download_{event_type}\")\n",
        "            if log_file_path:\n",
        "                print(f\"Detailed log saved: {log_file_path}\")\n",
        "\n",
        "    log_dl_start_data = {\"stage\": \"dl_process_start_ph4\", \"ts_utc\": datetime.now(timezone.utc).isoformat(timespec='seconds').replace('+00:00', 'Z'), \"user_conf_limit\": user_confirmed_download_limit_global, \"tasks_summary\": tasks_to_process_orig[['Account', 'Search_Query_Input', 'Tweet_Type_Desc', 'Estimated_Count']].to_dict(orient='records'), \"rate_limit_optimization\": \"disabled_fixed_1.1s\"}\n",
        "    log_operation_fn(log_dl_start_data, \"dl_process_start_ph4\")\n",
        "\n",
        "    if 'context_annotations' in TWEET_FIELDS_COMPREHENSIVE:\n",
        "        api_page_max_results = 100\n",
        "    else:\n",
        "        api_page_max_results = 500\n",
        "\n",
        "    # Фиксированная пауза\n",
        "    FIXED_PAUSE_SECONDS = 1.1\n",
        "\n",
        "    # Phase 4: Reset request counters\n",
        "    for token_id in clients_manager:\n",
        "        clients_manager[token_id]['request_count'] = 0\n",
        "\n",
        "    # Считаем общее количество твитов для скачивания\n",
        "    total_tweets_to_download = min(\n",
        "        tasks_to_process_orig['Estimated_Count'].sum(),\n",
        "        user_confirmed_download_limit_global\n",
        "    )\n",
        "\n",
        "    # Добавляем структуру для сбора статистики\n",
        "    download_statistics = {}  # {account: {tweet_type: count}}\n",
        "\n",
        "    with tqdm(total=total_tweets_to_download, desc=\"Overall Download Progress (Ph4)\", unit=\"tweets\") as pbar_overall_dl_tasks:\n",
        "        pbar_overall_dl_tasks.set_postfix_str(f\"0/{total_tweets_to_download} tweets\")\n",
        "        for task_idx, current_task_series_data in tasks_to_process_orig.iterrows():\n",
        "            if total_tweets_downloaded_run >= user_confirmed_download_limit_global:\n",
        "                update_status_display_fn([\"Global DL limit for this run reached.\"]); break\n",
        "\n",
        "            task_short_name = current_task_series_data.get('Task_Display_Name', f\"{current_task_series_data['Account'][:10]}_{current_task_series_data['Query_Tag_For_Path'][:10]}_{current_task_series_data['Tweet_Type_Slug_For_Path'][:10]}\")\n",
        "            pbar_overall_dl_tasks.set_postfix_str(task_short_name)\n",
        "\n",
        "            # Проверяем, это batch-строка или обычная\n",
        "            is_batch = 'Accounts_List' in current_task_series_data and current_task_series_data['Accounts_List']\n",
        "\n",
        "            if is_batch:\n",
        "                # Batch режим - обрабатываем каждый аккаунт отдельно\n",
        "                accounts_in_batch = current_task_series_data['Accounts_List']\n",
        "                total_tweets_for_batch = 0\n",
        "\n",
        "                # Обновляем общее количество аккаунтов для batch\n",
        "                current_status['total_accounts'] = len(accounts_in_batch)\n",
        "                current_status['accounts_processed'] = 0\n",
        "                current_status['current_type'] = current_task_series_data['Tweet_Type_Desc']\n",
        "\n",
        "                for acc_index, account in enumerate(accounts_in_batch, 1):\n",
        "                    if total_tweets_downloaded_run >= user_confirmed_download_limit_global:\n",
        "                        break\n",
        "\n",
        "                    # Обновляем статус\n",
        "                    current_status['current_account'] = account\n",
        "                    current_status['accounts_processed'] = acc_index\n",
        "                    current_status['tweets_downloaded'] = total_tweets_downloaded_run\n",
        "                    update_download_status()\n",
        "\n",
        "                    # Логируем начало обработки аккаунта\n",
        "                    log_detailed_event('accounts_processing', {\n",
        "                        'account': account,\n",
        "                        'position': f\"{acc_index}/{len(accounts_in_batch)}\",\n",
        "                        'tweet_type': current_task_series_data['Tweet_Type_Desc'],\n",
        "                        'tweets_so_far': total_tweets_downloaded_run\n",
        "                    })\n",
        "\n",
        "                    # Создаем отдельный запрос для каждого аккаунта\n",
        "                    single_account_query = build_full_search_query_fn(\n",
        "                        account,\n",
        "                        current_task_series_data['Tweet_Type_Filter_API'],\n",
        "                        current_task_series_data['Search_Query_Input']\n",
        "                    )\n",
        "\n",
        "                    # Увеличиваем лимит на аккаунт\n",
        "                    account_limit = min(5000, user_confirmed_download_limit_global - total_tweets_downloaded_run)\n",
        "                    tweets_for_account = 0\n",
        "\n",
        "                    # Стандартная логика скачивания для одного аккаунта\n",
        "                    active_dl_client_inst, token_id_dl_used_str = get_next_available_client_fn()\n",
        "                    if not active_dl_client_inst:\n",
        "                        update_status_display_fn([f\"No client available for {account}, skipping\"])\n",
        "                        continue\n",
        "\n",
        "                    try:\n",
        "                        time.sleep(FIXED_PAUSE_SECONDS)\n",
        "\n",
        "                        # Подготавливаем путь, но НЕ создаем директорию сразу\n",
        "                        fimi_slug_path = get_fimi_slug(current_task_series_data['FIMI_Event'])\n",
        "                        save_directory_path = os.path.join(\n",
        "                            ACTORS_BASE_DIR,\n",
        "                            fimi_slug_path,\n",
        "                            account.lstrip('@'),\n",
        "                            current_task_series_data['Query_Tag_For_Path'],\n",
        "                            current_task_series_data['Tweet_Type_Slug_For_Path']\n",
        "                        )\n",
        "                        # Флаг для отслеживания, создана ли директория\n",
        "                        directory_created = False\n",
        "\n",
        "                        # Пагинация для аккаунта\n",
        "                        tweet_paginator_instance = tweepy.Paginator(\n",
        "                            active_dl_client_inst.search_all_tweets,\n",
        "                            query=single_account_query,\n",
        "                            start_time=current_task_series_data['Start_Time_API'],\n",
        "                            end_time=current_task_series_data['End_Time_API'],\n",
        "                            max_results=api_page_max_results,\n",
        "                            tweet_fields=TWEET_FIELDS_COMPREHENSIVE,\n",
        "                            expansions=EXPANSIONS_COMPREHENSIVE,\n",
        "                            user_fields=USER_FIELDS_COMPREHENSIVE,\n",
        "                            media_fields=MEDIA_FIELDS_COMPREHENSIVE,\n",
        "                            poll_fields=POLL_FIELDS_COMPREHENSIVE,\n",
        "                            place_fields=PLACE_FIELDS_COMPREHENSIVE\n",
        "                        )\n",
        "\n",
        "                        pages_fetched = 0\n",
        "                        for page_response_obj in tweet_paginator_instance:\n",
        "                            if tweets_for_account >= account_limit or total_tweets_downloaded_run >= user_confirmed_download_limit_global:\n",
        "                                break\n",
        "\n",
        "                            pages_fetched += 1\n",
        "                            # Extract headers\n",
        "                            page_headers = {}\n",
        "                            if hasattr(page_response_obj, '_headers'):\n",
        "                                page_headers = dict(page_response_obj._headers)\n",
        "                            elif hasattr(page_response_obj, 'response') and hasattr(page_response_obj.response, 'headers'):\n",
        "                                page_headers = dict(page_response_obj.response.headers)\n",
        "\n",
        "                            update_client_rate_limit_info_fn(token_id_dl_used_str, page_headers, f\"dl_batch_split_{account[:10]}\")\n",
        "\n",
        "                            # Convert includes to serializable format\n",
        "                            serializable_includes = {}\n",
        "                            if hasattr(page_response_obj, 'includes') and page_response_obj.includes:\n",
        "                                if hasattr(page_response_obj.includes, 'users') and page_response_obj.includes.users:\n",
        "                                    serializable_includes['users'] = [user.data for user in page_response_obj.includes.users]\n",
        "                                if hasattr(page_response_obj.includes, 'media') and page_response_obj.includes.media:\n",
        "                                    serializable_includes['media'] = [media.data for media in page_response_obj.includes.media]\n",
        "                                if hasattr(page_response_obj.includes, 'polls') and page_response_obj.includes.polls:\n",
        "                                    serializable_includes['polls'] = [poll.data for poll in page_response_obj.includes.polls]\n",
        "                                if hasattr(page_response_obj.includes, 'places') and page_response_obj.includes.places:\n",
        "                                    serializable_includes['places'] = [place.data for place in page_response_obj.includes.places]\n",
        "                                if hasattr(page_response_obj.includes, 'tweets') and page_response_obj.includes.tweets:\n",
        "                                    serializable_includes['tweets'] = [tweet.data for tweet in page_response_obj.includes.tweets]\n",
        "\n",
        "                            # Process tweets\n",
        "                            if page_response_obj.data:\n",
        "                                for tweet_obj_data_item in page_response_obj.data:\n",
        "                                    if tweets_for_account >= account_limit or total_tweets_downloaded_run >= user_confirmed_download_limit_global:\n",
        "                                        break\n",
        "\n",
        "                                    # Prepare attachment summary with details\n",
        "                                    attachment_summary_dict = {}\n",
        "                                    if 'attachments' in tweet_obj_data_item.data:\n",
        "                                        if 'media_keys' in tweet_obj_data_item.data['attachments']:\n",
        "                                            attachment_summary_dict['media_present'] = True\n",
        "                                            attachment_summary_dict['media_keys'] = tweet_obj_data_item.data['attachments']['media_keys']\n",
        "                                            media_details = []\n",
        "                                            for media_key in tweet_obj_data_item.data['attachments']['media_keys']:\n",
        "                                                for media_item in serializable_includes.get('media', []):\n",
        "                                                    if media_item.get('media_key') == media_key:\n",
        "                                                        media_details.append({\n",
        "                                                            'media_key': media_key,\n",
        "                                                            'type': media_item.get('type'),\n",
        "                                                            'url': media_item.get('url'),\n",
        "                                                            'preview_image_url': media_item.get('preview_image_url'),\n",
        "                                                            'alt_text': media_item.get('alt_text'),\n",
        "                                                            'variants': media_item.get('variants', [])\n",
        "                                                        })\n",
        "                                            if media_details:\n",
        "                                                attachment_summary_dict['media_details'] = media_details\n",
        "\n",
        "                                        if 'poll_ids' in tweet_obj_data_item.data['attachments']:\n",
        "                                            attachment_summary_dict['poll_present'] = True\n",
        "                                            attachment_summary_dict['poll_ids'] = tweet_obj_data_item.data['attachments']['poll_ids']\n",
        "                                            poll_details = []\n",
        "                                            for poll_id in tweet_obj_data_item.data['attachments']['poll_ids']:\n",
        "                                                for poll_item in serializable_includes.get('polls', []):\n",
        "                                                    if poll_item.get('id') == poll_id:\n",
        "                                                        poll_details.append({\n",
        "                                                            'poll_id': poll_id,\n",
        "                                                            'options': poll_item.get('options'),\n",
        "                                                            'voting_status': poll_item.get('voting_status'),\n",
        "                                                            'end_datetime': poll_item.get('end_datetime')\n",
        "                                                        })\n",
        "                                            if poll_details:\n",
        "                                                attachment_summary_dict['poll_details'] = poll_details\n",
        "\n",
        "                                    # Обработка context_annotations\n",
        "                                    context_annotations_summary = []\n",
        "                                    if 'context_annotations' in tweet_obj_data_item.data:\n",
        "                                        for annotation in tweet_obj_data_item.data['context_annotations']:\n",
        "                                            context_annotations_summary.append({\n",
        "                                                'domain': annotation.get('domain', {}),\n",
        "                                                'entity': annotation.get('entity', {})\n",
        "                                            })\n",
        "\n",
        "                                    current_tweet_data_dict = tweet_obj_data_item.data.copy()\n",
        "                                    if attachment_summary_dict:\n",
        "                                        current_tweet_data_dict['attachment_summary'] = attachment_summary_dict\n",
        "                                    if context_annotations_summary:\n",
        "                                        current_tweet_data_dict['context_annotations_summary'] = context_annotations_summary\n",
        "\n",
        "                                    serializable_meta = {}\n",
        "                                    if hasattr(page_response_obj, 'meta') and page_response_obj.meta:\n",
        "                                        serializable_meta = dict(page_response_obj.meta)\n",
        "\n",
        "                                    # Добавляем username автора для удобства\n",
        "                                    if 'author_id' in current_tweet_data_dict and serializable_includes.get('users'):\n",
        "                                        author_id = current_tweet_data_dict['author_id']\n",
        "                                        for user in serializable_includes['users']:\n",
        "                                            if user.get('id') == author_id:\n",
        "                                                current_tweet_data_dict['author_username'] = user.get('username', 'unknown')\n",
        "                                                current_tweet_data_dict['author_name'] = user.get('name', 'unknown')\n",
        "                                                break\n",
        "\n",
        "                                    # Сохраняем твит с полными данными\n",
        "                                    tweet_to_save_dict = {\n",
        "                                        \"tweet_data\": current_tweet_data_dict,\n",
        "                                        \"includes\": serializable_includes,\n",
        "                                        \"meta_on_fetch\": serializable_meta\n",
        "                                    }\n",
        "\n",
        "                                    tweet_id_val_str = str(tweet_obj_data_item.id)\n",
        "                                    # Создаем директорию только при сохранении первого твита\n",
        "                                    if not directory_created:\n",
        "                                        os.makedirs(save_directory_path, exist_ok=True)\n",
        "                                        directory_created = True\n",
        "\n",
        "                                    tweet_json_file_path = os.path.join(save_directory_path, f\"{tweet_id_val_str}.json\")\n",
        "\n",
        "                                    with open(tweet_json_file_path, 'w', encoding='utf-8') as f_tweet_json_out:\n",
        "                                        json.dump(tweet_to_save_dict, f_tweet_json_out, ensure_ascii=False, indent=2)\n",
        "\n",
        "                                    tweets_for_account += 1\n",
        "                                    total_tweets_downloaded_run += 1\n",
        "                                    total_tweets_for_batch += 1\n",
        "                                    # Обновляем прогресс-бар\n",
        "                                    pbar_overall_dl_tasks.update(1)\n",
        "                                    pbar_overall_dl_tasks.set_postfix_str(f\"{total_tweets_downloaded_run}/{total_tweets_to_download} tweets\")\n",
        "\n",
        "                                    # Собираем статистику\n",
        "                                    account_clean = account.lstrip('@')\n",
        "                                    if account_clean not in download_statistics:\n",
        "                                        download_statistics[account_clean] = {}\n",
        "                                    tweet_type = current_task_series_data['Tweet_Type_Desc']\n",
        "                                    if tweet_type not in download_statistics[account_clean]:\n",
        "                                        download_statistics[account_clean][tweet_type] = 0\n",
        "                                    download_statistics[account_clean][tweet_type] += 1\n",
        "\n",
        "                                    # Обновляем статус каждые 10 твитов\n",
        "                                    current_status['tweets_downloaded'] = total_tweets_downloaded_run\n",
        "                                    if total_tweets_downloaded_run % 10 == 0:\n",
        "                                        update_download_status()\n",
        "\n",
        "                            time.sleep(FIXED_PAUSE_SECONDS)\n",
        "\n",
        "                    except tweepy.TooManyRequests as tmr_dl:\n",
        "                        update_status_display_fn([f\"RL on {token_id_dl_used_str} during batch DL for {account}.\"])\n",
        "                        reset_unix_dl = tmr_dl.response.headers.get('x-rate-limit-reset')\n",
        "                        reset_dt_dl = datetime.fromtimestamp(int(reset_unix_dl), timezone.utc) if reset_unix_dl else datetime.now(timezone.utc) + timedelta(minutes=16)\n",
        "                        clients_manager[token_id_dl_used_str]['rate_limited_until'] = reset_dt_dl + timedelta(seconds=10)\n",
        "                        update_client_rate_limit_info_fn(token_id_dl_used_str, tmr_dl.response.headers, f\"dl_batch_tmr_{account[:10]}\")\n",
        "                        update_status_display_fn(_get_token_status_string_fn(), is_token_status_update=True)\n",
        "                        log_detailed_event('rate_limit_events', {\n",
        "                            'token': token_id_dl_used_str,\n",
        "                            'account': account,\n",
        "                            'reset_time': reset_dt_dl.isoformat()\n",
        "                        })\n",
        "                    except Exception as e:\n",
        "                        error_msg = f\"Error processing account {account}: {str(e)[:100]}\"\n",
        "                        update_status_display_fn([error_msg])\n",
        "                        log_detailed_event('errors_log', {\n",
        "                            'account': account,\n",
        "                            'error': str(e),\n",
        "                            'traceback': traceback.format_exc()\n",
        "                        })\n",
        "                        traceback.print_exc()\n",
        "                        continue\n",
        "\n",
        "                # Логируем результат batch\n",
        "                download_log_list.append({\n",
        "                    \"task_details\": current_task_series_data.to_dict(),\n",
        "                    \"attempted_to_fetch\": current_task_series_data['Estimated_Count'],\n",
        "                    \"actually_fetched\": total_tweets_for_batch,\n",
        "                    \"tweets_by_account\": {acc: \"processed\" for acc in accounts_in_batch},\n",
        "                    \"status\": f\"Batch processed: {total_tweets_for_batch} tweets from {len(accounts_in_batch)} accounts\",\n",
        "                    \"api_pages_fetched\": \"multiple\"\n",
        "                })\n",
        "\n",
        "                update_status_display_fn([f\"Batch complete: {total_tweets_for_batch} tweets from {len(accounts_in_batch)} accounts ({current_task_series_data['Tweet_Type_Desc']})\"])\n",
        "\n",
        "            else:\n",
        "                # ОБЫЧНАЯ ЗАДАЧА (не batch) - используем старую логику\n",
        "                current_status['total_accounts'] = 1\n",
        "                current_status['accounts_processed'] = 1\n",
        "                current_status['current_account'] = current_task_series_data['Account']\n",
        "                current_status['current_type'] = current_task_series_data['Tweet_Type_Desc']\n",
        "\n",
        "                limit_for_this_task = min(current_task_series_data['Estimated_Count'], user_confirmed_download_limit_global - total_tweets_downloaded_run)\n",
        "                if limit_for_this_task <= 0:\n",
        "                    pbar_overall_dl_tasks.update(1)\n",
        "                    continue\n",
        "\n",
        "                fimi_slug_path = get_fimi_slug(current_task_series_data['FIMI_Event'])\n",
        "                save_directory_path = os.path.join(ACTORS_BASE_DIR, fimi_slug_path, current_task_series_data['Account'], current_task_series_data['Query_Tag_For_Path'], current_task_series_data['Tweet_Type_Slug_For_Path'])\n",
        "                # НЕ создаем директорию здесь\n",
        "                directory_created_for_task = False\n",
        "\n",
        "                tweets_downloaded_for_this_task = 0\n",
        "                estimated_count_for_task = current_task_series_data['Estimated_Count']\n",
        "\n",
        "                update_status_display_fn([f\"Fetching up to {limit_for_this_task} for {current_task_series_data['Account']}... (Est: {estimated_count_for_task})\"])\n",
        "\n",
        "                task_retries_count = 0\n",
        "                max_task_retries_allowed = len(clients_manager) + 2\n",
        "                pagination_token_for_task_retry = None\n",
        "                pages_fetched_for_task = 0\n",
        "\n",
        "                while tweets_downloaded_for_this_task < limit_for_this_task and task_retries_count < max_task_retries_allowed:\n",
        "                    if total_tweets_downloaded_run >= user_confirmed_download_limit_global: break\n",
        "\n",
        "                    active_dl_client_inst, token_id_dl_used_str = get_next_available_client_fn()\n",
        "                    if not active_dl_client_inst:\n",
        "                        update_status_display_fn([\"No client available for DL task, breaking task.\"])\n",
        "                        break\n",
        "\n",
        "                    if task_retries_count > 0:\n",
        "                        update_status_display_fn([f\"Retrying {task_short_name} with {token_id_dl_used_str} (Attempt {task_retries_count + 1})...\"])\n",
        "\n",
        "                    remaining_for_task_in_this_attempt = int(limit_for_this_task - tweets_downloaded_for_this_task)\n",
        "                    if remaining_for_task_in_this_attempt <= 0: break\n",
        "\n",
        "                    try:\n",
        "                        # Фиксированная пауза\n",
        "                        time.sleep(FIXED_PAUSE_SECONDS)\n",
        "\n",
        "                        tweet_paginator_instance = tweepy.Paginator(\n",
        "                            active_dl_client_inst.search_all_tweets,\n",
        "                            query=current_task_series_data['Actual_API_Query_Used'],\n",
        "                            start_time=current_task_series_data['Start_Time_API'],\n",
        "                            end_time=current_task_series_data['End_Time_API'],\n",
        "                            max_results=api_page_max_results,\n",
        "                            tweet_fields=TWEET_FIELDS_COMPREHENSIVE,\n",
        "                            expansions=EXPANSIONS_COMPREHENSIVE,\n",
        "                            user_fields=USER_FIELDS_COMPREHENSIVE,\n",
        "                            media_fields=MEDIA_FIELDS_COMPREHENSIVE,\n",
        "                            poll_fields=POLL_FIELDS_COMPREHENSIVE,\n",
        "                            place_fields=PLACE_FIELDS_COMPREHENSIVE,\n",
        "                            next_token=pagination_token_for_task_retry\n",
        "                        )\n",
        "\n",
        "                        pagination_token_for_task_retry = None\n",
        "\n",
        "                        with tqdm(total=remaining_for_task_in_this_attempt, desc=f\"{current_task_series_data['Account']} ({token_id_dl_used_str})\", leave=False, unit=\"tw\", position=1) as pbar_tweets_in_task_ui:\n",
        "                            for page_response_obj in tweet_paginator_instance:\n",
        "                                pages_fetched_for_task += 1\n",
        "\n",
        "                                # Extract headers correctly\n",
        "                                page_headers = {}\n",
        "                                if hasattr(page_response_obj, '_headers'):\n",
        "                                    page_headers = dict(page_response_obj._headers)\n",
        "                                elif hasattr(page_response_obj, 'response') and hasattr(page_response_obj.response, 'headers'):\n",
        "                                    page_headers = dict(page_response_obj.response.headers)\n",
        "\n",
        "                                update_client_rate_limit_info_fn(token_id_dl_used_str, page_headers, f\"dl_search_page_{task_short_name[:10]}\")\n",
        "\n",
        "                                pagination_token_for_task_retry = page_response_obj.meta.get('next_token') if page_response_obj.meta else None\n",
        "\n",
        "                                # Convert includes to serializable format\n",
        "                                serializable_includes = {}\n",
        "                                if hasattr(page_response_obj, 'includes') and page_response_obj.includes:\n",
        "                                    if hasattr(page_response_obj.includes, 'users') and page_response_obj.includes.users:\n",
        "                                        serializable_includes['users'] = [user.data for user in page_response_obj.includes.users]\n",
        "                                    if hasattr(page_response_obj.includes, 'media') and page_response_obj.includes.media:\n",
        "                                        serializable_includes['media'] = [media.data for media in page_response_obj.includes.media]\n",
        "                                    if hasattr(page_response_obj.includes, 'polls') and page_response_obj.includes.polls:\n",
        "                                        serializable_includes['polls'] = [poll.data for poll in page_response_obj.includes.polls]\n",
        "                                    if hasattr(page_response_obj.includes, 'places') and page_response_obj.includes.places:\n",
        "                                        serializable_includes['places'] = [place.data for place in page_response_obj.includes.places]\n",
        "                                    if hasattr(page_response_obj.includes, 'tweets') and page_response_obj.includes.tweets:\n",
        "                                        serializable_includes['tweets'] = [tweet.data for tweet in page_response_obj.includes.tweets]\n",
        "\n",
        "                                if page_response_obj.data:\n",
        "                                    for tweet_obj_data_item in page_response_obj.data:\n",
        "                                        if tweets_downloaded_for_this_task >= limit_for_this_task or total_tweets_downloaded_run >= user_confirmed_download_limit_global: break\n",
        "\n",
        "                                        # Prepare attachment summary with details\n",
        "                                        attachment_summary_dict = {}\n",
        "                                        if 'attachments' in tweet_obj_data_item.data:\n",
        "                                            if 'media_keys' in tweet_obj_data_item.data['attachments']:\n",
        "                                                attachment_summary_dict['media_present'] = True\n",
        "                                                attachment_summary_dict['media_keys'] = tweet_obj_data_item.data['attachments']['media_keys']\n",
        "                                                media_details = []\n",
        "                                                for media_key in tweet_obj_data_item.data['attachments']['media_keys']:\n",
        "                                                    for media_item in serializable_includes.get('media', []):\n",
        "                                                        if media_item.get('media_key') == media_key:\n",
        "                                                            media_details.append({\n",
        "                                                                'media_key': media_key,\n",
        "                                                                'type': media_item.get('type'),\n",
        "                                                                'url': media_item.get('url'),\n",
        "                                                                'preview_image_url': media_item.get('preview_image_url'),\n",
        "                                                                'alt_text': media_item.get('alt_text'),\n",
        "                                                                'variants': media_item.get('variants', [])\n",
        "                                                            })\n",
        "                                                if media_details:\n",
        "                                                    attachment_summary_dict['media_details'] = media_details\n",
        "\n",
        "                                            if 'poll_ids' in tweet_obj_data_item.data['attachments']:\n",
        "                                                attachment_summary_dict['poll_present'] = True\n",
        "                                                attachment_summary_dict['poll_ids'] = tweet_obj_data_item.data['attachments']['poll_ids']\n",
        "                                                poll_details = []\n",
        "                                                for poll_id in tweet_obj_data_item.data['attachments']['poll_ids']:\n",
        "                                                    for poll_item in serializable_includes.get('polls', []):\n",
        "                                                        if poll_item.get('id') == poll_id:\n",
        "                                                            poll_details.append({\n",
        "                                                                'poll_id': poll_id,\n",
        "                                                                'options': poll_item.get('options'),\n",
        "                                                                'voting_status': poll_item.get('voting_status'),\n",
        "                                                                'end_datetime': poll_item.get('end_datetime')\n",
        "                                                            })\n",
        "                                                if poll_details:\n",
        "                                                    attachment_summary_dict['poll_details'] = poll_details\n",
        "\n",
        "                                        # Обработка context_annotations\n",
        "                                        context_annotations_summary = []\n",
        "                                        if 'context_annotations' in tweet_obj_data_item.data:\n",
        "                                            for annotation in tweet_obj_data_item.data['context_annotations']:\n",
        "                                                context_annotations_summary.append({\n",
        "                                                    'domain': annotation.get('domain', {}),\n",
        "                                                    'entity': annotation.get('entity', {})\n",
        "                                                })\n",
        "\n",
        "                                        current_tweet_data_dict = tweet_obj_data_item.data.copy()\n",
        "                                        if attachment_summary_dict:\n",
        "                                            current_tweet_data_dict['attachment_summary'] = attachment_summary_dict\n",
        "                                        if context_annotations_summary:\n",
        "                                            current_tweet_data_dict['context_annotations_summary'] = context_annotations_summary\n",
        "\n",
        "                                        serializable_meta = {}\n",
        "                                        if hasattr(page_response_obj, 'meta') and page_response_obj.meta:\n",
        "                                            serializable_meta = dict(page_response_obj.meta)\n",
        "\n",
        "                                        tweet_to_save_dict = {\n",
        "                                            \"tweet_data\": current_tweet_data_dict,\n",
        "                                            \"includes\": serializable_includes,\n",
        "                                            \"meta_on_fetch\": serializable_meta\n",
        "                                        }\n",
        "\n",
        "                                        tweet_id_val_str = str(tweet_obj_data_item.id)\n",
        "                                        # Создаем директорию только при первом сохранении\n",
        "                                        if not directory_created_for_task:\n",
        "                                            try:\n",
        "                                                os.makedirs(save_directory_path, exist_ok=True)\n",
        "                                                directory_created_for_task = True\n",
        "                                            except Exception as e_mkdir_dl:\n",
        "                                                update_status_display_fn([f\"Err mkdir DL {save_directory_path}: {str(e_mkdir_dl)[:30]}\"])\n",
        "                                                continue\n",
        "\n",
        "                                        tweet_json_file_path = os.path.join(save_directory_path, f\"{tweet_id_val_str}.json\")\n",
        "                                        with open(tweet_json_file_path, 'w', encoding='utf-8') as f_tweet_json_out:\n",
        "                                            json.dump(tweet_to_save_dict, f_tweet_json_out, ensure_ascii=False, indent=2)\n",
        "\n",
        "                                        tweets_downloaded_for_this_task += 1\n",
        "                                        total_tweets_downloaded_run += 1\n",
        "                                        pbar_tweets_in_task_ui.update(1)\n",
        "\n",
        "                                        # Обновляем общий прогресс-бар\n",
        "                                        pbar_overall_dl_tasks.update(1)\n",
        "                                        pbar_overall_dl_tasks.set_postfix_str(f\"{total_tweets_downloaded_run}/{total_tweets_to_download} tweets\")\n",
        "\n",
        "                                        # Собираем статистику\n",
        "                                        account_clean = current_task_series_data['Account'].lstrip('@')\n",
        "                                        if account_clean == \"GLOBAL_SEARCH\":\n",
        "                                            account_clean = \"GLOBAL_SEARCH\"\n",
        "                                        if account_clean not in download_statistics:\n",
        "                                            download_statistics[account_clean] = {}\n",
        "                                        tweet_type = current_task_series_data['Tweet_Type_Desc']\n",
        "                                        if tweet_type not in download_statistics[account_clean]:\n",
        "                                            download_statistics[account_clean][tweet_type] = 0\n",
        "                                        download_statistics[account_clean][tweet_type] += 1\n",
        "\n",
        "                                        # Обновляем общий статус\n",
        "                                        current_status['tweets_downloaded'] = total_tweets_downloaded_run\n",
        "                                        if total_tweets_downloaded_run % 10 == 0:\n",
        "                                            update_download_status()\n",
        "\n",
        "                                        pbar_tweets_in_task_ui.set_postfix_str(f\"Total: {total_tweets_downloaded_run}/{user_confirmed_download_limit_global} | Task: {tweets_downloaded_for_this_task}/{estimated_count_for_task} | Pause: {FIXED_PAUSE_SECONDS}s\")\n",
        "\n",
        "                                if tweets_downloaded_for_this_task >= limit_for_this_task or total_tweets_downloaded_run >= user_confirmed_download_limit_global: break\n",
        "\n",
        "                                # Фиксированная пауза между страницами\n",
        "                                time.sleep(FIXED_PAUSE_SECONDS)\n",
        "\n",
        "                        # Успешно завершили - выходим из retry цикла\n",
        "                        break\n",
        "\n",
        "                    except tweepy.TooManyRequests as tmr_dl:\n",
        "                        update_status_display_fn([f\"RL on {token_id_dl_used_str} during DL for {task_short_name}.\"])\n",
        "                        reset_unix_dl = tmr_dl.response.headers.get('x-rate-limit-reset')\n",
        "                        reset_dt_dl = datetime.fromtimestamp(int(reset_unix_dl), timezone.utc) if reset_unix_dl else datetime.now(timezone.utc) + timedelta(minutes=16)\n",
        "                        clients_manager[token_id_dl_used_str]['rate_limited_until'] = reset_dt_dl + timedelta(seconds=10)\n",
        "                        update_client_rate_limit_info_fn(token_id_dl_used_str, tmr_dl.response.headers, f\"dl_search_tmr_{task_short_name[:10]}\")\n",
        "                        update_status_display_fn(_get_token_status_string_fn(), is_token_status_update=True)\n",
        "                        task_retries_count += 1\n",
        "                        if task_retries_count >= max_task_retries_allowed:\n",
        "                            update_status_display_fn([f\"Max retries for {task_short_name}. Moving to next task.\"])\n",
        "                            break\n",
        "                    except Exception as e_dl:\n",
        "                        update_status_display_fn([f\"DL Err {task_short_name} w/{token_id_dl_used_str}: {str(e_dl)[:50]}\"])\n",
        "                        traceback.print_exc()\n",
        "                        break\n",
        "\n",
        "                status_for_log = \"OK\"\n",
        "                if tweets_downloaded_for_this_task < limit_for_this_task and total_tweets_downloaded_run < user_confirmed_download_limit_global:\n",
        "                    status_for_log = f\"Partial ({tweets_downloaded_for_this_task}/{limit_for_this_task} due to error/limit or no more data)\"\n",
        "                elif tweets_downloaded_for_this_task == 0:\n",
        "                    status_for_log = \"NoTweetsFetched_Or_Error\"\n",
        "\n",
        "                download_log_list.append({\n",
        "                    \"task_details\": current_task_series_data.to_dict(),\n",
        "                    \"attempted_to_fetch_for_task\": limit_for_this_task,\n",
        "                    \"actually_fetched_for_task\": tweets_downloaded_for_this_task,\n",
        "                    \"save_directory\": save_directory_path,\n",
        "                    \"status\": status_for_log,\n",
        "                    \"estimated_total_for_task\": estimated_count_for_task,\n",
        "                    \"api_pages_fetched\": pages_fetched_for_task\n",
        "                })\n",
        "                update_status_display_fn([f\"Task {current_task_series_data['Account'][:10]}: Downloaded {tweets_downloaded_for_this_task}/{estimated_count_for_task} (Est) in {pages_fetched_for_task} API calls. Total run: {total_tweets_downloaded_run}.\"])\n",
        "\n",
        "            # Не обновляем прогресс-бар здесь, так как теперь обновляем при каждом твите\n",
        "            if total_tweets_downloaded_run >= user_confirmed_download_limit_global: break\n",
        "\n",
        "    # Phase 4: Final token usage summary\n",
        "    token_usage_summary = {token_id: info['request_count'] for token_id, info in clients_manager.items()}\n",
        "\n",
        "    # Формируем красивый вывод статистики\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DOWNLOAD SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total tweets downloaded: {total_tweets_downloaded_run}\")\n",
        "    print(\"\\nBreakdown by account and type:\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    for account, types in sorted(download_statistics.items()):\n",
        "        account_total = sum(types.values())\n",
        "        print(f\"\\n{account}: {account_total} total\")\n",
        "        for tweet_type, count in sorted(types.items()):\n",
        "            print(f\"  - {tweet_type}: {count}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "    # Обновляем статус с итоговой информацией\n",
        "    summary_lines = [f\"Phase 4 DL complete. Total: {total_tweets_downloaded_run} tweets\"]\n",
        "    for account, types in sorted(download_statistics.items()):\n",
        "        account_total = sum(types.values())\n",
        "        summary_lines.append(f\"{account}: {account_total} tweets\")\n",
        "    update_status_display_fn(summary_lines)\n",
        "\n",
        "    # Добавляем статистику в логи\n",
        "    log_dl_complete_data = {\n",
        "        \"stage\": \"dl_complete_ph4\",\n",
        "        \"ts_utc\": datetime.now(timezone.utc).isoformat(timespec='seconds').replace('+00:00', 'Z'),\n",
        "        \"user_conf_limit\": user_confirmed_download_limit_global,\n",
        "        \"total_dl_this_run\": total_tweets_downloaded_run,\n",
        "        \"download_statistics\": download_statistics,  # Новое поле со статистикой\n",
        "        \"dl_summary_per_task\": download_log_list,\n",
        "        \"token_usage_final\": token_usage_summary\n",
        "    }\n",
        "    log_operation_fn(log_dl_complete_data, \"dl_complete_ph4\")\n",
        "    download_button_ui.disabled = True\n",
        "    estimation_output_area_ui.clear_output()\n",
        "    download_confirmation_dialog_output_ui.clear_output()\n",
        "\n",
        "@reset_button_ui.on_click\n",
        "def handle_reset_button_click_fn(b_reset_ui):\n",
        "    global estimation_results_df, user_id_cache, user_confirmed_download_limit_global\n",
        "    global task_selection_checkboxes_global, _status_lines, clients_manager\n",
        "    global _current_active_token, estimated_requests_per_task\n",
        "\n",
        "    # 1. Сброс всех полей ввода\n",
        "    account_names_input.value = 'XDevelopers,elonmusk'\n",
        "    search_queries_input.value = ''\n",
        "    fimi_dropdown.value = FIMI_EVENTS[0]\n",
        "    start_date_picker.value = default_start_date_val\n",
        "    end_date_picker.value = default_end_date_val\n",
        "\n",
        "    # 2. Сброс чек-боксов типов твитов\n",
        "    for cb_item_val in tweet_type_checkboxes_map_ui.values():\n",
        "        cb_item_val.value = True\n",
        "    select_all_tweet_types_cb_ui.value = True\n",
        "\n",
        "    # 3. Сброс чек-боксов выбора токенов  ← **исправленный блок**\n",
        "    for cb in token_selection_checkboxes.values():\n",
        "        cb.value = True\n",
        "\n",
        "    # 4. Остальные переключатели\n",
        "    rate_limit_optimization_checkbox.value = True  # Phase 4\n",
        "\n",
        "    # 5. Очистка состояний и интерфейса\n",
        "    status_html.value = \"<i>Form reset. Configure and estimate.</i>\"\n",
        "    _status_lines.clear()\n",
        "    update_status_display_fn([\"Form reset.\"])\n",
        "\n",
        "    estimation_output_area_ui.clear_output()\n",
        "    download_confirmation_dialog_output_ui.clear_output()\n",
        "    download_button_ui.disabled = True\n",
        "\n",
        "    # 6. Сброс внутренних данных\n",
        "    estimation_results_df = pd.DataFrame()\n",
        "    user_id_cache.clear()\n",
        "    user_confirmed_download_limit_global = 0\n",
        "    user_limit_input_widget_ui.value = 1000\n",
        "    task_selection_checkboxes_global.clear()\n",
        "    _current_active_token = None          # Phase 4\n",
        "    estimated_requests_per_task.clear()   # Phase 4\n",
        "\n",
        "    # 7. Сброс счётчиков rate-limit по токенам\n",
        "    if clients_manager:\n",
        "        for token_id_key_val in clients_manager:\n",
        "            clients_manager[token_id_key_val]['rate_limited_until'] = (\n",
        "                datetime.now(timezone.utc) - timedelta(seconds=1)\n",
        "            )\n",
        "            clients_manager[token_id_key_val]['request_count'] = 0  # Phase 4\n",
        "\n",
        "    # 8. Обновление статуса\n",
        "    update_status_display_fn(_get_token_status_string_fn(), is_token_status_update=True)\n",
        "    update_status_display_fn([\"Client rate limit statuses reset (available).\"])\n",
        "\n",
        "\n",
        "# --- UI Layout and Display ---\n",
        "input_section_ui_layout = VBox([\n",
        "    widgets.HTML(\"<h3>1. Configure Data Collection Parameters:</h3>\"),\n",
        "    token_selection_box,  # Добавлено\n",
        "    account_names_input,\n",
        "    search_queries_input,\n",
        "    fimi_dropdown,\n",
        "    widgets.HTML(\"<b>Select Date Range (UTC):</b>\"),\n",
        "    date_box,\n",
        "    widgets.HTML(\"<b>Select Tweet Types & Options:</b>\"),\n",
        "    tweet_types_box_layout_ui\n",
        "], layout=Layout(border='1px solid #B0BEC5', padding='15px', margin='10px', width='auto'))\n",
        "actions_section_ui_layout = VBox([widgets.HTML(\"<h3>2. Execute Actions:</h3>\"), action_buttons_box_ui, widgets.HTML(\"<b>Live Status Updates (Phase 4 - Color Coded):</b>\"), status_html], layout=Layout(border='1px solid #B0BEC5', padding='15px', margin='10px', width='auto'))\n",
        "results_section_ui_layout = VBox([widgets.HTML(\"<h3>3. Estimation Results & Download Confirmation:</h3>\"), estimation_output_area_ui, download_confirmation_dialog_output_ui], layout=Layout(border='1px solid #B0BEC5', padding='15px', margin='10px', width='auto'))\n",
        "ui_main_layout = VBox([input_section_ui_layout, actions_section_ui_layout, results_section_ui_layout])\n",
        "\n",
        "if clients_fully_initialized_flag and clients_manager and any(clients_manager):\n",
        "    display(ui_main_layout)\n",
        "    update_status_display_fn([\"Phase 4 Interface loaded. Smart rate limiting enabled.\", \"Configure parameters and click 'Estimate'.\"])\n",
        "    update_status_display_fn(_get_token_status_string_fn(), is_token_status_update=True)\n",
        "else:\n",
        "    display(IPHTML(\"<h2><span style='color:red;'>CRITICAL ERROR: NO Tweepy clients initialized.</span></h2><p>Token files missing/invalid or path issue. Check `KEYS_PATH` and token files. Re-run cell.</p>\"))"
      ],
      "metadata": {
        "id": "2K9W16kK1h3N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}